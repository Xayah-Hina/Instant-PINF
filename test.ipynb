{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup Datasets\n",
    "1. image_batch['image'] - cuda:0\n",
    "2. image_batch['image_idx'] - cuda:0"
   ],
   "id": "dd6133957f2dbe6f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-21T05:58:30.285038Z",
     "start_time": "2024-11-21T05:56:32.888983Z"
    }
   },
   "source": [
    "import src.dataloader\n",
    "import torch\n",
    "import pathlib\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "dataloader = src.dataloader.load_train_data(\n",
    "    dataset_dir=pathlib.Path(\"C:/Users/imeho/Documents/DataSets/InstantPINF/ScalarReal\"),\n",
    "    split=\"train\",\n",
    "    device=device,\n",
    "    frame_skip=1,\n",
    "    # exclude_batch_keys_from_device=[\"image\", \"image_idx\"],\n",
    "    exclude_batch_keys_from_device=[],\n",
    ")\n",
    "image_batch = next(iter(dataloader))\n",
    "\n",
    "print(f'image device: {image_batch[\"image\"].device}')\n",
    "print(f'image indices: {image_batch[\"image_idx\"].device}')\n",
    "memory_image = image_batch['image'].element_size() * image_batch['image'].numel()\n",
    "memory_indices = image_batch['image_idx'].element_size() * image_batch['image_idx'].numel()\n",
    "print(f'Memory of image: {memory_image / 1024 / 1024:.2f} MB')\n",
    "print(f'Memory of indices: {memory_indices / 1024 / 1024:.2f} MB')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Caching all \u001B[1;36m480\u001B[0m images.\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">480</span> images.\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "540f803986db4c6d9404f69f775205e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image device: cuda:0\n",
      "image indices: cuda:0\n",
      "Memory of image: 11390.62 MB\n",
      "Memory of indices: 0.00 MB\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup Hash Encoder\n",
    "1. xyzt_encoder(xyzt)"
   ],
   "id": "fb82653119c25203"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T05:58:37.117208Z",
     "start_time": "2024-11-21T05:58:32.865557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import src.encoder\n",
    "import taichi as ti\n",
    "import numpy as np\n",
    "\n",
    "ti.init(arch=ti.cuda)\n",
    "xyzt_encoder = src.encoder.HashEncoderHyFluid(\n",
    "    min_res=np.array([16, 16, 16, 16]),\n",
    "    max_res=np.array([256, 256, 256, 128]),\n",
    "    num_scales=16,\n",
    "    max_params=2 ** 19,\n",
    ")\n",
    "xyzt_encoder.to(device)"
   ],
   "id": "9b3ce0d74e08fe53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.7.2, llvm 15.0.1, commit 0131dce9, win, python 3.11.0\n",
      "[Taichi] Starting on arch=cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HashEncoderHyFluid()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup Model & Optimizer\n",
    "1. mlp(xyzt_encoded)\n",
    "2. learned_rgb\n",
    "3. optimizer"
   ],
   "id": "3b375e94d2190507"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T05:58:40.569290Z",
     "start_time": "2024-11-21T05:58:39.623069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nerfstudio.field_components.mlp\n",
    "import src.radam\n",
    "\n",
    "mlp = nerfstudio.field_components.mlp.MLP(\n",
    "    in_dim=xyzt_encoder.num_scales * xyzt_encoder.features_per_level,\n",
    "    num_layers=2,\n",
    "    layer_width=64,\n",
    "    out_dim=1,\n",
    "    out_activation=torch.nn.ReLU(),\n",
    ").to(device)\n",
    "learned_rgb = torch.nn.Parameter(torch.tensor([0.0], device=device))\n",
    "\n",
    "grad_vars = list(mlp.parameters()) + [learned_rgb]\n",
    "embedding_params = list(xyzt_encoder.parameters())\n",
    "\n",
    "optimizer = src.radam.RAdam([\n",
    "    {'params': grad_vars, 'weight_decay': 1e-6},\n",
    "    {'params': embedding_params, 'eps': 1e-15}\n",
    "], lr=0.01, betas=(0.9, 0.99))"
   ],
   "id": "b9858af38cf14b12",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup Ray Sampler & Collider",
   "id": "dd326ffdc61683f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T06:43:56.734409Z",
     "start_time": "2024-11-21T06:43:56.713676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nerfstudio.model_components.ray_generators\n",
    "import nerfstudio.model_components.ray_samplers\n",
    "import nerfstudio.data.pixel_samplers\n",
    "import nerfstudio.model_components.scene_colliders\n",
    "\n",
    "pixel_sampler = nerfstudio.data.pixel_samplers.PixelSamplerConfig(num_rays_per_batch=4096).setup()\n",
    "ray_generator = nerfstudio.model_components.ray_generators.RayGenerator(dataloader.dataset.cameras).to(device)\n",
    "uniform_sampler = nerfstudio.model_components.ray_samplers.UniformSampler(num_samples=192).to(device)\n",
    "near_far_collider = nerfstudio.model_components.scene_colliders.NearFarCollider(near_plane=1.1, far_plane=1.5).to(\n",
    "    device)"
   ],
   "id": "48aa0d5f36df12ed",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Optimization Loop",
   "id": "3c9b0ffc7f7b9d02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T06:44:24.810400Z",
     "start_time": "2024-11-21T06:44:13.763881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "import nerfstudio.model_components.losses\n",
    "\n",
    "raw2alpha = lambda raw, dists, act_fn=torch.nn.functional.relu: 1. - torch.exp(-act_fn(raw) * dists)\n",
    "rgb_loss = nerfstudio.model_components.losses.MSELoss()\n",
    "\n",
    "\n",
    "def perturb_frames_sample(image_batch, all_frames, batch):\n",
    "    sample_image = batch['image']\n",
    "    sample_idx = batch['indices'].float()\n",
    "\n",
    "    video_starts = [0] + list(torch.cumsum(torch.tensor(all_frames[:-1]), dim=0).numpy())\n",
    "    video_ends = [start + frames - 1 for start, frames in zip(video_starts, all_frames)]\n",
    "\n",
    "    images = image_batch['image']\n",
    "    image_idx = image_batch['image_idx']\n",
    "    num_samples = sample_image.shape[0]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        current_frame_idx = int(sample_idx[i, 0].item())\n",
    "\n",
    "        # 检查是否为视频的第一个或最后一个帧\n",
    "        is_first_or_last_frame = any(\n",
    "            current_frame_idx == video_starts[j] or current_frame_idx == video_ends[j]\n",
    "            for j in range(len(all_frames))\n",
    "        )\n",
    "        if is_first_or_last_frame:\n",
    "            continue\n",
    "\n",
    "        current_height_idx = int(sample_idx[i, 1].item())\n",
    "        current_width_idx = int(sample_idx[i, 2].item())\n",
    "        perturb_value = (torch.rand(1).item() - 0.5)\n",
    "\n",
    "        if perturb_value > 0:\n",
    "            # 获取下一帧的索引\n",
    "            next_frame_idx = torch.where(image_idx == (current_frame_idx + 1))[0].item()\n",
    "            assert image_idx[next_frame_idx] == (current_frame_idx + 1)\n",
    "            weight_curr = (0.5 - perturb_value) / 0.5\n",
    "            weight_next = perturb_value / 0.5\n",
    "            sample_image[i] = weight_curr * sample_image[i] + weight_next * images[\n",
    "                next_frame_idx, current_height_idx, current_width_idx]\n",
    "            sample_idx[i, 0] = current_frame_idx + perturb_value\n",
    "\n",
    "        elif perturb_value < 0:\n",
    "            # 获取前一帧的索引\n",
    "            prev_frame_idx = torch.where(image_idx == (current_frame_idx - 1))[0].item()\n",
    "            assert image_idx[prev_frame_idx] == (current_frame_idx - 1)\n",
    "            weight_curr = (0.5 + perturb_value) / 0.5\n",
    "            weight_prev = -perturb_value / 0.5\n",
    "            sample_image[i] = weight_curr * sample_image[i] + weight_prev * images[\n",
    "                prev_frame_idx, current_height_idx, current_width_idx]\n",
    "            sample_idx[i, 0] = current_frame_idx + perturb_value\n",
    "\n",
    "    batch['image'] = sample_image\n",
    "    batch['indices'] = sample_idx\n",
    "\n",
    "\n",
    "# 定义一个字典存储每句代码的执行时间\n",
    "execution_times = defaultdict(list)\n",
    "# 用于记录每次循环的 loss 值\n",
    "loss_values = []\n",
    "\n",
    "# 模拟循环任务\n",
    "for i in tqdm.tqdm(range(50000)):\n",
    "    # 记录每一步的执行时间\n",
    "    start = time.time()\n",
    "    batch = pixel_sampler.sample(image_batch)\n",
    "    execution_times[\"sample\"].append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    ray_bundle = near_far_collider(ray_generator(batch['indices']))\n",
    "    execution_times[\"ray_bundle\"].append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    ray_samples_uniform = uniform_sampler(ray_bundle)\n",
    "    execution_times[\"ray_samples_uniform\"].append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    positions = ray_samples_uniform.frustums.get_positions()\n",
    "    execution_times[\"get_positions\"].append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    frames = batch['indices'][..., 0] % 120\n",
    "    execution_times[\"frames_mod\"].append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    perturb_frames_sample(image_batch, dataloader.dataset.metadata['all_frames'], batch)\n",
    "    execution_times[\"perturb_frames_sample\"].append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    frames_expanded = frames.unsqueeze(1).unsqueeze(2).expand(-1, positions.shape[1], -1)\n",
    "    execution_times[\"frames_expand\"].append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    xyzt = torch.cat([positions, frames_expanded], dim=-1)\n",
    "    execution_times[\"concat\"].append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    xyzt_flat = xyzt.reshape(-1, 4)\n",
    "    execution_times[\"reshape\"].append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    xyzt_encoded = xyzt_encoder(xyzt_flat)\n",
    "    execution_times[\"xyzt_encoder\"].append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    raw_flat = mlp(xyzt_encoded)\n",
    "    execution_times[\"mlp\"].append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    raw = raw_flat.reshape(xyzt.shape[0], xyzt.shape[1], raw_flat.shape[-1])\n",
    "    execution_times[\"raw_reshape\"].append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    dists = ray_samples_uniform.deltas\n",
    "    rgb = torch.ones(3, device=device) * (0.6 + torch.tanh(learned_rgb) * 0.4)\n",
    "    alpha = raw2alpha(raw[..., -1], dists[..., -1])\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1), device=device), 1. - alpha + 1e-10], -1),\n",
    "                                    -1)[:, :-1]\n",
    "    rgb_map = torch.sum(weights[..., None] * rgb, -2)\n",
    "    execution_times[\"raw2alpha\"].append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    loss = rgb_loss(rgb_map, batch['image'])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    new_lrate = 5e-4 * (0.1 ** (i / 250))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lrate\n",
    "    execution_times[\"optimizing\"].append(time.time() - start)\n",
    "\n",
    "    # 记录 loss 值\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "# 计算平均耗时并排序\n",
    "avg_execution_times = {key: sum(times) / len(times) for key, times in execution_times.items()}\n",
    "sorted_avg_times = sorted(avg_execution_times.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 获取平均耗时 Top 5 的步骤\n",
    "top5_keys = [item[0] for item in sorted_avg_times[:5]]\n",
    "\n",
    "# 绘制折线图\n",
    "plt.figure(figsize=(12, 8))\n",
    "for key in top5_keys:\n",
    "    plt.plot(range(1, len(execution_times[key]) + 1), execution_times[key], label=key, marker='o')\n",
    "\n",
    "# 设置图表标题和轴标签\n",
    "plt.title(\"Top 5 Execution Times per Step\", fontsize=14)\n",
    "plt.xlabel(\"Iteration\", fontsize=12)\n",
    "plt.ylabel(\"Time (seconds)\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 绘制 loss 曲线\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(range(1, len(loss_values) + 1), loss_values, label=\"Loss\", color=\"red\", marker='o')\n",
    "plt.title(\"Loss Curve\", fontsize=14)\n",
    "plt.xlabel(\"Iteration\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "c94acdf84d753673",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/50000 [00:10<6:06:24,  2.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 137\u001B[0m\n\u001B[0;32m    134\u001B[0m     execution_times[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptimizing\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start)\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;66;03m# 记录 loss 值\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     loss_values\u001B[38;5;241m.\u001B[39mappend(\u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    139\u001B[0m \u001B[38;5;66;03m# 计算平均耗时并排序\u001B[39;00m\n\u001B[0;32m    140\u001B[0m avg_execution_times \u001B[38;5;241m=\u001B[39m {key: \u001B[38;5;28msum\u001B[39m(times) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(times) \u001B[38;5;28;01mfor\u001B[39;00m key, times \u001B[38;5;129;01min\u001B[39;00m execution_times\u001B[38;5;241m.\u001B[39mitems()}\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
