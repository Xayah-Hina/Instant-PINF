{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:27:05.278466Z",
     "start_time": "2024-12-04T16:26:58.778199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "pinf_data = np.load(\"train_dataset.npz\")\n",
    "IMAGE_TRAIN_np = pinf_data['images_train']\n",
    "POSES_TRAIN_np = pinf_data['poses_train']\n",
    "HWF_np = pinf_data['hwf']\n",
    "RENDER_POSE_np = pinf_data['render_poses']\n",
    "RENDER_TIMESTEPs_np = pinf_data['render_timesteps']\n",
    "VOXEL_TRAN_np = pinf_data['voxel_tran']\n",
    "VOXEL_SCALE_np = pinf_data['voxel_scale']\n",
    "NEAR_float = pinf_data['near'].item()\n",
    "FAR_float = pinf_data['far'].item()\n",
    "\n",
    "del pinf_data\n",
    "print(f'IMAGE_TRAIN_np.shape: {IMAGE_TRAIN_np.shape}')\n",
    "print(f'POSES_TRAIN_np.shape: {POSES_TRAIN_np.shape}')\n",
    "print(f'HWF_np: {HWF_np}')\n",
    "print(f'RENDER_POSE_np.shape: {RENDER_POSE_np.shape}')\n",
    "print(f'RENDER_TIMESTEPs_np.shape: {RENDER_TIMESTEPs_np.shape}')\n",
    "print(f'VOXEL_TRAN_np.shape: {VOXEL_TRAN_np.shape}')\n",
    "print(f'VOXEL_SCALE_np.shape: {VOXEL_SCALE_np.shape}')\n",
    "print(f'NEAR_float: {NEAR_float}')\n",
    "print(f'FAR_float: {FAR_float}')"
   ],
   "id": "ac53fc5476050fa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_TRAIN_np.shape: (120, 4, 960, 540, 3)\n",
      "POSES_TRAIN_np.shape: (4, 4, 4)\n",
      "HWF_np: [ 960.      540.     1306.8817]\n",
      "RENDER_POSE_np.shape: (120, 4, 4)\n",
      "RENDER_TIMESTEPs_np.shape: (120,)\n",
      "VOXEL_TRAN_np.shape: (4, 4)\n",
      "VOXEL_SCALE_np.shape: (3,)\n",
      "NEAR_float: 1.1\n",
      "FAR_float: 1.5\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:27:09.973136Z",
     "start_time": "2024-12-04T16:27:05.279129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import taichi as ti\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "ti.init(arch=ti.cuda, device_memory_GB=8.0)"
   ],
   "id": "16bd4890b6547ee7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.7.2, llvm 15.0.1, commit 0131dce9, win, python 3.11.0\n",
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:27:10.173477Z",
     "start_time": "2024-12-04T16:27:10.165567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args_npz = np.load(\"args.npz\", allow_pickle=True)\n",
    "ARGs = SimpleNamespace(**{\n",
    "    key: value.item() if isinstance(value, np.ndarray) and value.size == 1 else\n",
    "    value.tolist() if isinstance(value, np.ndarray) else\n",
    "    value\n",
    "    for key, value in args_npz.items()\n",
    "})\n",
    "del args_npz\n",
    "\n",
    "print(type(ARGs))"
   ],
   "id": "6a27483695aabe39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'types.SimpleNamespace'>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:27:11.105939Z",
     "start_time": "2024-12-04T16:27:10.229719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from encoder import HashEncoderHyFluid\n",
    "\n",
    "ENCODER = HashEncoderHyFluid(\n",
    "    min_res=np.array([ARGs.base_resolution, ARGs.base_resolution, ARGs.base_resolution, ARGs.base_resolution_t]),\n",
    "    max_res=np.array(\n",
    "        [ARGs.finest_resolution, ARGs.finest_resolution, ARGs.finest_resolution, ARGs.finest_resolution_t]),\n",
    "    num_scales=ARGs.num_levels,\n",
    "    max_params=2 ** ARGs.log2_hashmap_size).to(device)\n",
    "ENCODER_params = list(ENCODER.parameters())"
   ],
   "id": "7b84d679d62f3120",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imeho\\Desktop\\241118\\Instant-PINF\\test\\encoder.py:275: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "C:\\Users\\imeho\\Desktop\\241118\\Instant-PINF\\test\\encoder.py:298: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "C:\\Users\\imeho\\Desktop\\241118\\Instant-PINF\\test\\encoder.py:310: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "C:\\Users\\imeho\\Desktop\\241118\\Instant-PINF\\test\\encoder.py:336: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:27:11.316320Z",
     "start_time": "2024-12-04T16:27:11.307376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeRFSmall(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers=3,\n",
    "                 hidden_dim=64,\n",
    "                 geo_feat_dim=15,\n",
    "                 num_layers_color=2,\n",
    "                 hidden_dim_color=16,\n",
    "                 input_ch=3,\n",
    "                 ):\n",
    "        super(NeRFSmall, self).__init__()\n",
    "\n",
    "        self.input_ch = input_ch\n",
    "        self.rgb = torch.nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        # sigma network\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.geo_feat_dim = geo_feat_dim\n",
    "\n",
    "        sigma_net = []\n",
    "        for l in range(num_layers):\n",
    "            if l == 0:\n",
    "                in_dim = self.input_ch\n",
    "            else:\n",
    "                in_dim = hidden_dim\n",
    "\n",
    "            if l == num_layers - 1:\n",
    "                out_dim = 1  # 1 sigma + 15 SH features for color\n",
    "            else:\n",
    "                out_dim = hidden_dim\n",
    "\n",
    "            sigma_net.append(torch.nn.Linear(in_dim, out_dim, bias=False))\n",
    "\n",
    "        self.sigma_net = torch.nn.ModuleList(sigma_net)\n",
    "\n",
    "        self.color_net = []\n",
    "        for l in range(num_layers_color):\n",
    "            if l == 0:\n",
    "                in_dim = 1\n",
    "            else:\n",
    "                in_dim = hidden_dim_color\n",
    "\n",
    "            if l == num_layers_color - 1:\n",
    "                out_dim = 1\n",
    "            else:\n",
    "                out_dim = hidden_dim_color\n",
    "\n",
    "            self.color_net.append(torch.nn.Linear(in_dim, out_dim, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.sigma_net[l](h)\n",
    "            h = torch.nn.functional.relu(h, inplace=True)\n",
    "\n",
    "        sigma = h\n",
    "        return sigma\n",
    "\n",
    "\n",
    "MODEL = NeRFSmall(num_layers=2,\n",
    "                  hidden_dim=64,\n",
    "                  geo_feat_dim=15,\n",
    "                  num_layers_color=2,\n",
    "                  hidden_dim_color=16,\n",
    "                  input_ch=ENCODER.num_scales * 2).to(device)\n",
    "GRAD_vars = list(MODEL.parameters())"
   ],
   "id": "918abf0a3c5a50c4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:27:12.756998Z",
     "start_time": "2024-12-04T16:27:11.337413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "\n",
    "        self.degenerated_to_sgd = degenerated_to_sgd\n",
    "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
    "            for param in params:\n",
    "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
    "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
    "                        buffer=[[None, None, None] for _ in range(10)])\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = group['buffer'][int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt(\n",
    "                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
    "                                    N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    elif self.degenerated_to_sgd:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = -1\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(p_data_fp32, alpha=-group['weight_decay'] * group['lr'])\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "                elif step_size > 0:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(p_data_fp32, alpha=-group['weight_decay'] * group['lr'])\n",
    "                    p_data_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "optimizer = RAdam([\n",
    "    {'params': GRAD_vars, 'weight_decay': 1e-6},\n",
    "    {'params': ENCODER_params, 'eps': 1e-15}\n",
    "], lr=ARGs.lrate, betas=(0.9, 0.99))\n",
    "GRAD_vars += list(ENCODER_params)\n",
    "\n",
    "print(f'len(GRAD_vars): {len(GRAD_vars)}')"
   ],
   "id": "be59514cdb778e42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(GRAD_vars): 4\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:27:12.908187Z",
     "start_time": "2024-12-04T16:27:12.781674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pos_world2smoke(Pworld, w2s, scale_vector):\n",
    "    pos_rot = torch.sum(Pworld[..., None, :] * (w2s[:3, :3]), -1)  # 4.world to 3.target\n",
    "    pos_off = (w2s[:3, -1]).expand(pos_rot.shape)  # 4.world to 3.target\n",
    "    new_pose = pos_rot + pos_off\n",
    "    pos_scale = new_pose / (scale_vector)  # 3.target to 2.simulation\n",
    "    return pos_scale\n",
    "\n",
    "\n",
    "class BBox_Tool(object):\n",
    "    def __init__(self, smoke_tran_inv, smoke_scale, in_min=[0.15, 0.0, 0.15], in_max=[0.85, 1., 0.85]):\n",
    "        self.s_w2s = torch.tensor(smoke_tran_inv, device=device, dtype=torch.float32).expand([4, 4])\n",
    "        self.s2w = torch.inverse(self.s_w2s)\n",
    "        self.s_scale = torch.tensor(smoke_scale.copy(), device=device, dtype=torch.float32).expand([3])\n",
    "        self.s_min = torch.tensor(in_min, device=device, dtype=torch.float32)\n",
    "        self.s_max = torch.tensor(in_max, device=device, dtype=torch.float32)\n",
    "\n",
    "    def world2sim(self, pts_world):\n",
    "        pts_world_homo = torch.cat([pts_world, torch.ones_like(pts_world[..., :1])], dim=-1)\n",
    "        pts_sim_ = torch.matmul(self.s_w2s, pts_world_homo[..., None]).squeeze(-1)[..., :3]\n",
    "        pts_sim = pts_sim_ / (self.s_scale)  # 3.target to 2.simulation\n",
    "        return pts_sim\n",
    "\n",
    "    def world2sim_rot(self, pts_world):\n",
    "        pts_sim_ = torch.matmul(self.s_w2s[:3, :3], pts_world[..., None]).squeeze(-1)\n",
    "        pts_sim = pts_sim_ / (self.s_scale)  # 3.target to 2.simulation\n",
    "        return pts_sim\n",
    "\n",
    "    def sim2world(self, pts_sim):\n",
    "        pts_sim_ = pts_sim * self.s_scale\n",
    "        pts_sim_homo = torch.cat([pts_sim_, torch.ones_like(pts_sim_[..., :1])], dim=-1)\n",
    "        pts_world = torch.matmul(self.s2w, pts_sim_homo[..., None]).squeeze(-1)[..., :3]\n",
    "        return pts_world\n",
    "\n",
    "    def sim2world_rot(self, pts_sim):\n",
    "        pts_sim_ = pts_sim * self.s_scale\n",
    "        pts_world = torch.matmul(self.s2w[:3, :3], pts_sim_[..., None]).squeeze(-1)\n",
    "        return pts_world\n",
    "\n",
    "    def isInside(self, inputs_pts):\n",
    "        target_pts = pos_world2smoke(inputs_pts, self.s_w2s, self.s_scale)\n",
    "        above = torch.logical_and(target_pts[..., 0] >= self.s_min[0], target_pts[..., 1] >= self.s_min[1])\n",
    "        above = torch.logical_and(above, target_pts[..., 2] >= self.s_min[2])\n",
    "        below = torch.logical_and(target_pts[..., 0] <= self.s_max[0], target_pts[..., 1] <= self.s_max[1])\n",
    "        below = torch.logical_and(below, target_pts[..., 2] <= self.s_max[2])\n",
    "        outputs = torch.logical_and(below, above)\n",
    "        return outputs\n",
    "\n",
    "    def insideMask(self, inputs_pts, to_float=True):\n",
    "        return self.isInside(inputs_pts).to(torch.float) if to_float else self.isInside(inputs_pts)\n",
    "\n",
    "\n",
    "voxel_tran_inv = np.linalg.inv(VOXEL_TRAN_np)\n",
    "BBOX_MODEL_gpu = BBox_Tool(voxel_tran_inv, VOXEL_SCALE_np)"
   ],
   "id": "877337c83f6d76cc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:27:12.933967Z",
     "start_time": "2024-12-04T16:27:12.929677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "H = int(HWF_np[0])\n",
    "W = int(HWF_np[1])\n",
    "FOCAL = float(HWF_np[2])\n",
    "K = np.array([[FOCAL, 0, 0.5 * W], [0, FOCAL, 0.5 * H], [0, 0, 1]])\n",
    "print(f'H: {H}, W: {W}, FOCAL: {FOCAL}, K: {K}')"
   ],
   "id": "2aa518864920e5e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H: 960, W: 540, FOCAL: 1306.8817138671875, K: [[1.30688171e+03 0.00000000e+00 2.70000000e+02]\n",
      " [0.00000000e+00 1.30688171e+03 4.80000000e+02]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:27:12.961364Z",
     "start_time": "2024-12-04T16:27:12.956959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# namespace(\n",
    "#     config='configs/scalarflowreal.txt',\n",
    "#     expname='exp_real/density_256_128',\n",
    "#     basedir='./logs',\n",
    "#     datadir='./data/ScalarReal',\n",
    "#     encoder='ingp',\n",
    "#     N_rand=256,\n",
    "#     N_time=1,\n",
    "#     lrate=0.01,\n",
    "#     lrate_decay=10000,\n",
    "#     N_iters=10000,\n",
    "#     no_reload=False,\n",
    "#     ft_path=None,\n",
    "#     N_samples=192,\n",
    "#     perturb=1.0,\n",
    "#     render_only=False,\n",
    "#     half_res=True,\n",
    "#     i_print=100,\n",
    "#     i_weights=10000,\n",
    "#     i_video=10000,\n",
    "#     finest_resolution=256,\n",
    "#     finest_resolution_t=128,\n",
    "#     num_levels=16,\n",
    "#     base_resolution=16,\n",
    "#     base_resolution_t=16,\n",
    "#     log2_hashmap_size=19,\n",
    "#     feats_dim=36,\n",
    "#     tv_loss_weight=1e-06\n",
    "# )"
   ],
   "id": "242c3fb2f72985d8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:28:48.245089Z",
     "start_time": "2024-12-04T16:27:12.983237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_rays_np_continuous(c2w):\n",
    "    i, j = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy')\n",
    "    random_offset_i = np.random.uniform(0, 1, size=(H, W))\n",
    "    random_offset_j = np.random.uniform(0, 1, size=(H, W))\n",
    "    i = i + random_offset_i\n",
    "    j = j + random_offset_j\n",
    "    i = np.clip(i, 0, W - 1)\n",
    "    j = np.clip(j, 0, H - 1)\n",
    "\n",
    "    dirs = np.stack([(i - K[0][2]) / K[0][0], -(j - K[1][2]) / K[1][1], -np.ones_like(i)], -1)\n",
    "    # Rotate ray directions from camera frame to the world frame\n",
    "    rays_d = np.sum(dirs[..., np.newaxis, :] * c2w[:3, :3],\n",
    "                    -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n",
    "    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n",
    "    rays_o = np.broadcast_to(c2w[:3, -1], np.shape(rays_d))\n",
    "    return rays_o, rays_d, i, j\n",
    "\n",
    "def sample_bilinear(img, xy):\n",
    "    \"\"\"\n",
    "    Sample image with bilinear interpolation\n",
    "    :param img: (T, V, H, W, 3)\n",
    "    :param xy: (V, 2, H, W)\n",
    "    :return: img: (T, V, H, W, 3)\n",
    "    \"\"\"\n",
    "    T, V, H, W, _ = img.shape\n",
    "    u, v = xy[:, 0], xy[:, 1]\n",
    "\n",
    "    u = np.clip(u, 0, W - 1)\n",
    "    v = np.clip(v, 0, H - 1)\n",
    "\n",
    "    u_floor, v_floor = np.floor(u).astype(int), np.floor(v).astype(int)\n",
    "    u_ceil, v_ceil = np.ceil(u).astype(int), np.ceil(v).astype(int)\n",
    "\n",
    "    u_ratio, v_ratio = u - u_floor, v - v_floor\n",
    "    u_ratio, v_ratio = u_ratio[None, ..., None], v_ratio[None, ..., None]\n",
    "\n",
    "    bottom_left = img[:, np.arange(V)[:, None, None], v_floor, u_floor]\n",
    "    bottom_right = img[:, np.arange(V)[:, None, None], v_floor, u_ceil]\n",
    "    top_left = img[:, np.arange(V)[:, None, None], v_ceil, u_floor]\n",
    "    top_right = img[:, np.arange(V)[:, None, None], v_ceil, u_ceil]\n",
    "\n",
    "    bottom = (1 - u_ratio) * bottom_left + u_ratio * bottom_right\n",
    "    top = (1 - u_ratio) * top_left + u_ratio * top_right\n",
    "\n",
    "    interpolated = (1 - v_ratio) * bottom + v_ratio * top\n",
    "\n",
    "    return interpolated\n",
    "\n",
    "\n",
    "def do_resample_rays():\n",
    "    rays_list = []\n",
    "    ij = []\n",
    "    for p in POSES_TRAIN_np[:, :3, :4]:\n",
    "        r_o, r_d, i_, j_ = get_rays_np_continuous(p)\n",
    "        rays_list.append([r_o, r_d])\n",
    "        ij.append([i_, j_])\n",
    "    ij = np.stack(ij, 0)\n",
    "    images_train_sample = sample_bilinear(IMAGE_TRAIN_np, ij)\n",
    "    ret_IMAGE_TRAIN_gpu = torch.tensor(images_train_sample, device=device, dtype=torch.float32).flatten(start_dim=1,\n",
    "                                                                                                        end_dim=3)\n",
    "\n",
    "    rays_np = np.stack(rays_list, 0)\n",
    "    rays_np = np.transpose(rays_np, [0, 2, 3, 1, 4])\n",
    "    rays_np = np.reshape(rays_np, [-1, 2, 3])  # [VHW, ro+rd=2, 3]\n",
    "    rays_np = rays_np.astype(np.float32)\n",
    "    ret_RAYs_gpu = torch.tensor(rays_np, device=device, dtype=torch.float32)\n",
    "    ret_RAY_IDX_gpu = torch.randperm(ret_RAYs_gpu.shape[0], device=device, dtype=torch.int32)\n",
    "\n",
    "    return ret_IMAGE_TRAIN_gpu, ret_RAYs_gpu, ret_RAY_IDX_gpu\n",
    "\n",
    "\n",
    "IMAGE_TRAIN_gpu, RAYs_gpu, RAY_IDX_gpu = do_resample_rays()\n",
    "resample_rays = False\n",
    "print(f'IMAGE_TRAIN_gpu: shape={IMAGE_TRAIN_gpu.shape}, dtype={IMAGE_TRAIN_gpu.dtype}, device={IMAGE_TRAIN_gpu.device}')\n",
    "print(f'RAYs_gpu: shape={RAYs_gpu.shape}, dtype={RAYs_gpu.dtype}, device={RAYs_gpu.device}')\n",
    "print(f'RAY_IDX_gpu: shape={RAY_IDX_gpu.shape}, dtype={RAY_IDX_gpu.dtype}, device={RAY_IDX_gpu.device}')"
   ],
   "id": "109bb2083373d417",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_TRAIN_gpu: shape=torch.Size([120, 2073600, 3]), dtype=torch.float32, device=cuda:0\n",
      "RAYs_gpu: shape=torch.Size([2073600, 2, 3]), dtype=torch.float32, device=cuda:0\n",
      "RAY_IDX_gpu: shape=torch.Size([2073600]), dtype=torch.int32, device=cuda:0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:28:49.676918Z",
     "start_time": "2024-12-04T16:28:49.650640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_ray_batch(RAYs: torch.Tensor, RAYs_IDX: torch.Tensor, start: int, end: int):\n",
    "    BATCH_RAYs_IDX = RAYs_IDX[start:end]  # [batch_size]\n",
    "    BATCH_RAYs_O, BATCH_RAYs_D = torch.transpose(RAYs[BATCH_RAYs_IDX], 0, 1)  # [batch_size, 3]\n",
    "    return BATCH_RAYs_O, BATCH_RAYs_D, BATCH_RAYs_IDX\n",
    "\n",
    "\n",
    "def get_frames_at_times(IMAGEs: torch.Tensor, N_frames: int, N_times: int):\n",
    "    assert N_frames > 1\n",
    "    TIMEs_IDX = torch.randperm(N_frames, device=IMAGEs.device, dtype=torch.float32)[:N_times] + torch.randn(N_times, device=IMAGEs.device, dtype=torch.float32)  # [N_times]\n",
    "    TIMEs_IDX_FLOOR = torch.clamp(torch.floor(TIMEs_IDX).long(), 0, N_frames - 1)  # [N_times]\n",
    "    TIMEs_IDX_CEIL = torch.clamp(torch.ceil(TIMEs_IDX).long(), 0, N_frames - 1)  # [N_times]\n",
    "    TIMEs_IDX_RESIDUAL = TIMEs_IDX - TIMEs_IDX_FLOOR.float()  # [N_times]\n",
    "    TIME_STEPs = TIMEs_IDX / (N_frames - 1)  # [N_times]\n",
    "\n",
    "    FRAMES_INTERPOLATED = IMAGEs[TIMEs_IDX_FLOOR] * (1 - TIMEs_IDX_RESIDUAL).view(-1, 1, 1) + IMAGEs[TIMEs_IDX_CEIL] * TIMEs_IDX_RESIDUAL.view(-1, 1, 1)\n",
    "    return FRAMES_INTERPOLATED, TIME_STEPs\n",
    "\n",
    "\n",
    "def get_points(RAYs_O: torch.Tensor, RAYs_D: torch.Tensor, near: float, far: float, N_depths: int, randomize: bool):\n",
    "    T_VALs = torch.linspace(0., 1., steps=N_depths, device=RAYs_D.device, dtype=torch.float32)  # [N_depths]\n",
    "    Z_VALs = near * torch.ones_like(RAYs_D[..., :1]) * (1. - T_VALs) + far * torch.ones_like(RAYs_D[..., :1]) * T_VALs  # [batch_size, N_depths]\n",
    "\n",
    "    if randomize:\n",
    "        MID_VALs = .5 * (Z_VALs[..., 1:] + Z_VALs[..., :-1])  # [batch_size, N_depths-1]\n",
    "        UPPER_VALs = torch.cat([MID_VALs, Z_VALs[..., -1:]], -1)  # [batch_size, N_depths]\n",
    "        LOWER_VALs = torch.cat([Z_VALs[..., :1], MID_VALs], -1)  # [batch_size, N_depths]\n",
    "        T_RAND = torch.rand(Z_VALs.shape, device=RAYs_D.device, dtype=torch.float32)  # [batch_size, N_depths]\n",
    "        Z_VALs = LOWER_VALs + (UPPER_VALs - LOWER_VALs) * T_RAND  # [batch_size, N_depths]\n",
    "\n",
    "    DIST_VALs = Z_VALs[..., 1:] - Z_VALs[..., :-1]  # [batch_size, N_depths-1]\n",
    "    POINTS = RAYs_O[..., None, :] + RAYs_D[..., None, :] * Z_VALs[..., :, None]  # [batch_size, N_depths, 3]\n",
    "    return POINTS, DIST_VALs\n",
    "\n",
    "\n",
    "def get_raw(POINTS_TIME: torch.Tensor, DISTs: torch.Tensor, RAYs_D: torch.Tensor):\n",
    "    POINTS_TIME_FLAT = POINTS_TIME.view(-1, POINTS_TIME.shape[-1])  # [batch_size * N_depths, 4]\n",
    "    out_dim = 1\n",
    "    RAW_FLAT = torch.zeros([POINTS_TIME_FLAT.shape[0], out_dim], device=POINTS_TIME_FLAT.device, dtype=torch.float32)\n",
    "    bbox_mask = BBOX_MODEL_gpu.insideMask(POINTS_TIME_FLAT[..., :3], to_float=False)\n",
    "    if bbox_mask.sum() == 0:\n",
    "        bbox_mask[0] = True\n",
    "    POINTS_TIME_FLAT_FINAL = POINTS_TIME_FLAT[bbox_mask]\n",
    "\n",
    "    RAW_FLAT[bbox_mask] = MODEL(ENCODER(POINTS_TIME_FLAT_FINAL))\n",
    "    RAW = RAW_FLAT.reshape(*POINTS_TIME.shape[:-1], out_dim)\n",
    "\n",
    "    DISTs_cat = torch.cat([DISTs, torch.tensor([1e10], device=DISTs.device).expand(DISTs[..., :1].shape)], -1)  # [batch_size, N_depths]\n",
    "    DISTS_final = DISTs_cat * torch.norm(RAYs_D[..., None, :], dim=-1)  # [batch_size, N_depths]\n",
    "    print(f'DISTs.shape: {DISTs.shape}, DISTs_cat.shape: {DISTs_cat.shape}, DISTS_final.shape: {DISTS_final.shape}')\n",
    "\n",
    "    RGB_TRAINED = torch.ones(3, device=POINTS_TIME.device) * (0.6 + torch.tanh(MODEL.rgb) * 0.4)\n",
    "    raw2alpha = lambda raw, dists, act_fn=torch.nn.functional.relu: 1. - torch.exp(-act_fn(raw) * dists)\n",
    "    noise = 0.\n",
    "    alpha = raw2alpha(RAW[..., -1] + noise, DISTS_final)\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1), device=device), 1. - alpha + 1e-10], -1), -1)[:, :-1]\n",
    "    rgb_map = torch.sum(weights[..., None] * RGB_TRAINED, -2)\n",
    "    return rgb_map"
   ],
   "id": "94eb28cf7ff0e365",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:29:38.864748Z",
     "start_time": "2024-12-04T16:29:38.849869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tqdm\n",
    "total = RAY_IDX_gpu.shape[0]\n",
    "batch_size = 256\n",
    "time_size = 1\n",
    "depth_size = 192\n",
    "# for i in tqdm.trange(0, total, batch_size):\n",
    "#     BATCH_RAYs_O_gpu, BATCH_RAYs_D_gpu, BATCH_RAYs_IDX_gpu = get_ray_batch(RAYs_gpu, RAY_IDX_gpu, i, i + batch_size)  # [batch_size, 3], [batch_size, 3], [batch_size]\n",
    "#     FRAMES_INTERPOLATED_gpu, TIME_STEPs_gpu = get_frames_at_times(IMAGE_TRAIN_gpu, IMAGE_TRAIN_gpu.shape[0], time_size)  # [N_times, N x H x W, 3], [N_times]\n",
    "#     TARGET_S_gpu = FRAMES_INTERPOLATED_gpu[:, BATCH_RAYs_IDX_gpu].flatten(0, 1)  # [batch_size * N_times, 3]\n",
    "#     POINTS_gpu, DISTs_gpu = get_points(BATCH_RAYs_O_gpu, BATCH_RAYs_D_gpu, NEAR_float, FAR_float, depth_size, randomize=True)  # [batch_size, N_depths, 3]\n",
    "#     for TIME_STEP_gpu in TIME_STEPs_gpu:\n",
    "#         POINTS_TIME_gpu = torch.cat([POINTS_gpu, TIME_STEP_gpu.expand(POINTS_gpu[..., :1].shape)], dim=-1)  # [batch_size, N_depths, 4]\n",
    "#         RGB_MAP = get_raw(POINTS_TIME_gpu, DISTs_gpu, BATCH_RAYs_D_gpu)"
   ],
   "id": "b4b819a1fe19d730",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:29:49.950111Z",
     "start_time": "2024-12-04T16:29:40.464539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import os\n",
    "\n",
    "pinf_data_test = np.load(\"test_dataset.npz\")\n",
    "IMAGE_TEST_np = pinf_data_test['images_test']\n",
    "POSES_TEST_np = pinf_data_test['poses_test']\n",
    "\n",
    "print(f'IMAGE_TEST_np.shape: {IMAGE_TEST_np.shape}')\n",
    "print(f'POSES_TEST_np.shape: {POSES_TEST_np.shape}')\n",
    "del pinf_data_test\n",
    "\n",
    "\n",
    "def get_rays(H, W, K, c2w):\n",
    "    i, j = torch.meshgrid(torch.linspace(0, W - 1, W, device=device), torch.linspace(0, H - 1, H, device=device),\n",
    "                          indexing='ij')  # pytorch's meshgrid has indexing='ij'\n",
    "    i = i.t()\n",
    "    j = j.t()\n",
    "    dirs = torch.stack([(i - K[0][2]) / K[0][0], -(j - K[1][2]) / K[1][1], -torch.ones_like(i)], -1)\n",
    "    # Rotate ray directions from camera frame to the world frame\n",
    "    rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3, :3],\n",
    "                       -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n",
    "    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n",
    "    rays_o = c2w[:3, -1].expand(rays_d.shape)\n",
    "    return rays_o, rays_d\n",
    "\n",
    "def get_raw_my(POINTS_TIME: torch.Tensor, DISTs: torch.Tensor, RAYs_D_FLAT: torch.Tensor):\n",
    "    assert POINTS_TIME.dim() == 3 and POINTS_TIME.shape[-1] == 4\n",
    "    assert POINTS_TIME.shape[0] == DISTs.shape[0] == RAYs_D_FLAT.shape[0]\n",
    "    POINTS_TIME_FLAT = POINTS_TIME.view(-1, POINTS_TIME.shape[-1])  # [batch_size * N_depths, 4]\n",
    "    out_dim = 1\n",
    "    RAW_FLAT = torch.zeros([POINTS_TIME_FLAT.shape[0], out_dim], device=POINTS_TIME_FLAT.device, dtype=torch.float32)\n",
    "    bbox_mask = BBOX_MODEL_gpu.insideMask(POINTS_TIME_FLAT[..., :3], to_float=False)\n",
    "    if bbox_mask.sum() == 0:\n",
    "        bbox_mask[0] = True\n",
    "        assert False\n",
    "    POINTS_TIME_FLAT_FINAL = POINTS_TIME_FLAT[bbox_mask]\n",
    "\n",
    "    for _ in tqdm.trange(0, POINTS_TIME_FLAT_FINAL.shape[0], batch_size):\n",
    "        RAW_FLAT[bbox_mask][_:_ + batch_size] = MODEL(ENCODER(POINTS_TIME_FLAT_FINAL[_:_ + batch_size]))\n",
    "    RAW = RAW_FLAT.reshape(*POINTS_TIME.shape[:-1], out_dim)\n",
    "    assert RAW.dim() == 3 and RAW.shape[-1] == 1\n",
    "\n",
    "    DISTs_cat = torch.cat([DISTs, torch.tensor([1e10], device=DISTs.device).expand(DISTs[..., :1].shape)], -1)  # [batch_size, N_depths]\n",
    "    DISTS_final = DISTs_cat * torch.norm(RAYs_D_FLAT[..., None, :], dim=-1)  # [batch_size, N_depths]\n",
    "    print(f'DISTs.shape: {DISTs.shape}, DISTs_cat.shape: {DISTs_cat.shape}, DISTS_final.shape: {DISTS_final.shape}')\n",
    "\n",
    "    RGB_TRAINED = torch.ones(3, device=POINTS_TIME.device) * (0.6 + torch.tanh(MODEL.rgb) * 0.4)\n",
    "    raw2alpha = lambda raw, dists, act_fn=torch.nn.functional.relu: 1. - torch.exp(-act_fn(raw) * dists)\n",
    "    noise = 0.\n",
    "    alpha = raw2alpha(RAW[..., -1] + noise, DISTS_final)\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1), device=device), 1. - alpha + 1e-10], -1), -1)[:, :-1]\n",
    "    rgb_map = torch.sum(weights[..., None] * RGB_TRAINED, -2)\n",
    "    return rgb_map\n",
    "\n",
    "# new_shape = (-1, *POINTS_TIME.shape[-2:])\n",
    "# POINTS_TIME = POINTS_TIME.view(new_shape)\n",
    "\n",
    "os.makedirs(os.path.join(\"output\"), exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    test_view_pose = torch.tensor(POSES_TEST_np[0], device=device, dtype=torch.float32)\n",
    "    N_timesteps = IMAGE_TEST_np.shape[0]\n",
    "    test_timesteps = torch.arange(N_timesteps, device=device) / (N_timesteps - 1)\n",
    "\n",
    "    c2w = test_view_pose\n",
    "    rays_o, rays_d = get_rays(H, W, K, c2w)\n",
    "    points, dists = get_points(rays_o, rays_d, NEAR_float, FAR_float, 192, randomize=False)\n",
    "\n",
    "    points_flat = points.flatten(0, 1)\n",
    "    dists_flatt = dists.flatten(0, 1)\n",
    "    rays_d_flatt = rays_d.flatten(0, 1)\n",
    "\n",
    "\n",
    "    for i in tqdm.trange(0, test_timesteps.shape[0]):\n",
    "        test_timesteps_expended = test_timesteps[i].expand(points_flat[..., :1].shape)\n",
    "        points_time_flat_gpu = torch.cat([points_flat, test_timesteps_expended], dim=-1)\n",
    "        rgb_map = get_raw_my(points_time_flat_gpu, dists_flatt, rays_d_flatt)\n",
    "    print(f'rgb_map.shape: {rgb_map.shape}')"
   ],
   "id": "1788ac5ce51a25e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_TEST_np.shape: (120, 1, 960, 540, 3)\n",
      "POSES_TEST_np.shape: (1, 4, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120 [00:00<?, ?it/s]\n",
      "  0%|          | 0/157373 [00:02<?, ?it/s]\u001B[A\n",
      "  0%|          | 0/120 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[taichi/rhi/cuda/cuda_driver.h:taichi::lang::CUDADriverFunction<void * *,unsigned __int64,void *>::operator ()@92] CUDA Error CUDA_ERROR_OUT_OF_MEMORY: out of memory while calling malloc_async_impl (cuMemAllocAsync)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 78\u001B[0m\n\u001B[0;32m     76\u001B[0m     test_timesteps_expended \u001B[38;5;241m=\u001B[39m test_timesteps[i]\u001B[38;5;241m.\u001B[39mexpand(points_flat[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, :\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     77\u001B[0m     points_time_flat_gpu \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([points_flat, test_timesteps_expended], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 78\u001B[0m     rgb_map \u001B[38;5;241m=\u001B[39m \u001B[43mget_raw_my\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpoints_time_flat_gpu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdists_flatt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrays_d_flatt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrgb_map.shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrgb_map\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[16], line 41\u001B[0m, in \u001B[0;36mget_raw_my\u001B[1;34m(POINTS_TIME, DISTs, RAYs_D_FLAT)\u001B[0m\n\u001B[0;32m     38\u001B[0m POINTS_TIME_FLAT_FINAL \u001B[38;5;241m=\u001B[39m POINTS_TIME_FLAT[bbox_mask]\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m tqdm\u001B[38;5;241m.\u001B[39mtrange(\u001B[38;5;241m0\u001B[39m, POINTS_TIME_FLAT_FINAL\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], batch_size):\n\u001B[1;32m---> 41\u001B[0m     RAW_FLAT[bbox_mask][_:_ \u001B[38;5;241m+\u001B[39m batch_size] \u001B[38;5;241m=\u001B[39m MODEL(\u001B[43mENCODER\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPOINTS_TIME_FLAT_FINAL\u001B[49m\u001B[43m[\u001B[49m\u001B[43m_\u001B[49m\u001B[43m:\u001B[49m\u001B[43m_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     42\u001B[0m RAW \u001B[38;5;241m=\u001B[39m RAW_FLAT\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m*\u001B[39mPOINTS_TIME\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], out_dim)\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m RAW\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m RAW\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\Documents\\VituralEnvs\\HyFluid\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\VituralEnvs\\HyFluid\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\241118\\Instant-PINF\\test\\encoder.py:370\u001B[0m, in \u001B[0;36mHashEncoderHyFluid.forward\u001B[1;34m(self, positions)\u001B[0m\n\u001B[0;32m    367\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, positions):\n\u001B[0;32m    368\u001B[0m     \u001B[38;5;66;03m# positions: (N, 4), normalized to [-1, 1]\u001B[39;00m\n\u001B[0;32m    369\u001B[0m     positions \u001B[38;5;241m=\u001B[39m positions \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0.5\u001B[39m\n\u001B[1;32m--> 370\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodule_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpositions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhash_table\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\VituralEnvs\\HyFluid\\Lib\\site-packages\\torch\\autograd\\function.py:575\u001B[0m, in \u001B[0;36mFunction.apply\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m    572\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[0;32m    573\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[0;32m    574\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[1;32m--> 575\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    579\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    580\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    581\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    582\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    583\u001B[0m     )\n",
      "File \u001B[1;32m~\\Documents\\VituralEnvs\\HyFluid\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:476\u001B[0m, in \u001B[0;36mcustom_fwd.<locals>.decorate_fwd\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    471\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m fwd(\n\u001B[0;32m    472\u001B[0m             \u001B[38;5;241m*\u001B[39m_cast(args, device_type, cast_inputs),\n\u001B[0;32m    473\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_cast(kwargs, device_type, cast_inputs),\n\u001B[0;32m    474\u001B[0m         )\n\u001B[0;32m    475\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 476\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfwd\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\241118\\Instant-PINF\\test\\encoder.py:278\u001B[0m, in \u001B[0;36mHashEncoderHyFluid.__init__.<locals>.ModuleFunction.forward\u001B[1;34m(ctx, input_pos, params)\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;129m@custom_fwd\u001B[39m(cast_inputs\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(ctx, input_pos, params):\n\u001B[0;32m    277\u001B[0m     output_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_embedding[:input_pos\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m--> 278\u001B[0m     \u001B[43mtorch2ti\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_fields\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_pos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontiguous\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    279\u001B[0m     torch2ti(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparameter_fields, params\u001B[38;5;241m.\u001B[39mcontiguous())\n\u001B[0;32m    281\u001B[0m     hash_encode_kernel(\n\u001B[0;32m    282\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_fields,\n\u001B[0;32m    283\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparameter_fields,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    290\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_scales,\n\u001B[0;32m    291\u001B[0m     )\n",
      "File \u001B[1;32m~\\Documents\\VituralEnvs\\HyFluid\\Lib\\site-packages\\taichi\\lang\\kernel_impl.py:1113\u001B[0m, in \u001B[0;36m_kernel_impl.<locals>.wrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1110\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(_func)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1112\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1113\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mprimal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1114\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (TaichiCompilationError, TaichiRuntimeError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1115\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m impl\u001B[38;5;241m.\u001B[39mget_runtime()\u001B[38;5;241m.\u001B[39mprint_full_traceback:\n",
      "File \u001B[1;32m~\\Documents\\VituralEnvs\\HyFluid\\Lib\\site-packages\\taichi\\lang\\shell.py:27\u001B[0m, in \u001B[0;36m_shell_pop_print.<locals>.new_call\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(old_call)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnew_call\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 27\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mold_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;66;03m# print's in kernel won't take effect until ti.sync(), discussion:\u001B[39;00m\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;66;03m# https://github.com/taichi-dev/taichi/pull/1303#discussion_r444897102\u001B[39;00m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28mprint\u001B[39m(_ti_core\u001B[38;5;241m.\u001B[39mpop_python_print_buffer(), end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\VituralEnvs\\HyFluid\\Lib\\site-packages\\taichi\\lang\\kernel_impl.py:1043\u001B[0m, in \u001B[0;36mKernel.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1041\u001B[0m     _logging\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mopt_level = 1 is enforced to enable gradient computation.\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[0;32m   1042\u001B[0m     impl\u001B[38;5;241m.\u001B[39mcurrent_cfg()\u001B[38;5;241m.\u001B[39mopt_level \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1043\u001B[0m key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mensure_compiled\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1044\u001B[0m kernel_cpp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiled_kernels[key]\n\u001B[0;32m   1045\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunch_kernel(kernel_cpp, \u001B[38;5;241m*\u001B[39margs)\n",
      "File \u001B[1;32m~\\Documents\\VituralEnvs\\HyFluid\\Lib\\site-packages\\taichi\\lang\\kernel_impl.py:1011\u001B[0m, in \u001B[0;36mKernel.ensure_compiled\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1009\u001B[0m instance_id, arg_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmapper\u001B[38;5;241m.\u001B[39mlookup(args)\n\u001B[0;32m   1010\u001B[0m key \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc, instance_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautodiff_mode)\n\u001B[1;32m-> 1011\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaterialize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marg_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1012\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m key\n",
      "File \u001B[1;32m~\\Documents\\VituralEnvs\\HyFluid\\Lib\\site-packages\\taichi\\lang\\kernel_impl.py:637\u001B[0m, in \u001B[0;36mKernel.materialize\u001B[1;34m(self, key, args, arg_features)\u001B[0m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    636\u001B[0m     key \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautodiff_mode)\n\u001B[1;32m--> 637\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mruntime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaterialize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    639\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiled_kernels:\n\u001B[0;32m    640\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\VituralEnvs\\HyFluid\\Lib\\site-packages\\taichi\\lang\\impl.py:471\u001B[0m, in \u001B[0;36mPyTaichi.materialize\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    470\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmaterialize\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 471\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaterialize_root_fb\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaterialized\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    472\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaterialized \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate_fields_builder()\n",
      "File \u001B[1;32m~\\Documents\\VituralEnvs\\HyFluid\\Lib\\site-packages\\taichi\\lang\\impl.py:406\u001B[0m, in \u001B[0;36mPyTaichi.materialize_root_fb\u001B[1;34m(is_first_call)\u001B[0m\n\u001B[0;32m    403\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m root\u001B[38;5;241m.\u001B[39mfinalized:\n\u001B[0;32m    404\u001B[0m         root\u001B[38;5;241m.\u001B[39m_allocate_adjoint_checkbit()\n\u001B[1;32m--> 406\u001B[0m \u001B[43mroot\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraise_warning\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mis_first_call\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m _root_fb\n\u001B[0;32m    408\u001B[0m _root_fb \u001B[38;5;241m=\u001B[39m FieldsBuilder()\n",
      "File \u001B[1;32m~\\Documents\\VituralEnvs\\HyFluid\\Lib\\site-packages\\taichi\\_snode\\fields_builder.py:170\u001B[0m, in \u001B[0;36mFieldsBuilder.finalize\u001B[1;34m(self, raise_warning)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfinalize\u001B[39m(\u001B[38;5;28mself\u001B[39m, raise_warning\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    166\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Constructs the SNodeTree and finalizes this builder.\u001B[39;00m\n\u001B[0;32m    167\u001B[0m \n\u001B[0;32m    168\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;124;03m        raise_warning (bool): Raise warning or not.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 170\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_finalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraise_warning\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompile_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\VituralEnvs\\HyFluid\\Lib\\site-packages\\taichi\\_snode\\fields_builder.py:182\u001B[0m, in \u001B[0;36mFieldsBuilder._finalize\u001B[1;34m(self, raise_warning, compile_only)\u001B[0m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinalized \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    181\u001B[0m impl\u001B[38;5;241m.\u001B[39mget_runtime()\u001B[38;5;241m.\u001B[39mfinalize_fields_builder(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m--> 182\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m SNodeTree(\u001B[43m_ti_core\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinalize_snode_tree\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_snode_registry\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mptr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_runtime\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompile_only\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: [taichi/rhi/cuda/cuda_driver.h:taichi::lang::CUDADriverFunction<void * *,unsigned __int64,void *>::operator ()@92] CUDA Error CUDA_ERROR_OUT_OF_MEMORY: out of memory while calling malloc_async_impl (cuMemAllocAsync)"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
