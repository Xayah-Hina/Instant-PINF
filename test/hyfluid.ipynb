{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Laod Data",
   "id": "bdbb67e8cd299f6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T10:07:41.025212Z",
     "start_time": "2024-11-25T10:07:37.952327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import taichi as ti\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio.v2 as imageio\n",
    "import tqdm\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import lpips\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ],
   "id": "5a754d26dee3c831",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.7.2, llvm 15.0.1, commit 0131dce9, win, python 3.11.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'impo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m \u001B[43mimpo\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Optimizer\n\u001B[0;32m     14\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'impo' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 加载数据\n",
    "pinf_data = np.load(\"data/ScalarReal.npz\")\n",
    "images_train_ = pinf_data['images_train']\n",
    "poses_train = pinf_data['poses_train']\n",
    "hwf = pinf_data['hwf']\n",
    "render_poses = pinf_data['render_poses']\n",
    "render_timesteps = pinf_data['render_timesteps']\n",
    "voxel_tran = pinf_data['voxel_tran']\n",
    "voxel_scale = pinf_data['voxel_scale']\n",
    "near = pinf_data['near']\n",
    "far = pinf_data['far']\n",
    "\n",
    "pinf_data_test = np.load(\"data/ScalarRealTest.npz\")\n",
    "images_test = pinf_data_test['images_test']\n",
    "poses_test = pinf_data_test['poses_test']\n",
    "\n",
    "# 创建表格数据\n",
    "data = {\n",
    "    \"Name\": [\n",
    "        \"images_train\",\n",
    "        \"poses_train\",\n",
    "        \"hwf\",\n",
    "        \"render_poses\",\n",
    "        \"render_timesteps\",\n",
    "        \"voxel_tran\",\n",
    "        \"voxel_scale\",\n",
    "        \"near\",\n",
    "        \"far\",\n",
    "        \"images_test\",\n",
    "        \"poses_test\",\n",
    "    ],\n",
    "    \"Shape/Value\": [\n",
    "        images_train_.shape,\n",
    "        poses_train.shape,\n",
    "        hwf.shape,\n",
    "        render_poses.shape,\n",
    "        render_timesteps.shape,\n",
    "        voxel_tran.shape,\n",
    "        voxel_scale.shape,\n",
    "        near,\n",
    "        far,\n",
    "        images_test.shape,\n",
    "        poses_test.shape,\n",
    "    ],\n",
    "    \"Size (MB)\": [\n",
    "        images_train_.nbytes / 1024 ** 2,\n",
    "        poses_train.nbytes / 1024 ** 2,\n",
    "        hwf.nbytes / 1024 ** 2,\n",
    "        render_poses.nbytes / 1024 ** 2,\n",
    "        render_timesteps.nbytes / 1024 ** 2,\n",
    "        voxel_tran.nbytes / 1024 ** 2,\n",
    "        voxel_scale.nbytes / 1024 ** 2,\n",
    "        None,\n",
    "        None,\n",
    "        images_test.nbytes / 1024 ** 2,\n",
    "        poses_test.nbytes / 1024 ** 2,\n",
    "    ],\n",
    "}\n",
    "\n",
    "# 创建 DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 显示 DataFrame\n",
    "df.style.set_table_attributes(\"style='display:inline'\").set_caption(\"Data Summary\")"
   ],
   "id": "75753396309a382d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def pos_world2smoke(Pworld, w2s, scale_vector):\n",
    "    pos_rot = torch.sum(Pworld[..., None, :] * (w2s[:3, :3]), -1)  # 4.world to 3.target\n",
    "    pos_off = (w2s[:3, -1]).expand(pos_rot.shape)  # 4.world to 3.target\n",
    "    new_pose = pos_rot + pos_off\n",
    "    pos_scale = new_pose / (scale_vector)  # 3.target to 2.simulation\n",
    "    return pos_scale\n",
    "\n",
    "\n",
    "class BBox_Tool(object):\n",
    "    def __init__(self, smoke_tran_inv, smoke_scale, in_min=[0.15, 0.0, 0.15], in_max=[0.85, 1., 0.85]):\n",
    "        self.s_w2s = torch.tensor(smoke_tran_inv).expand([4, 4]).float()\n",
    "        self.s2w = torch.inverse(self.s_w2s)\n",
    "        self.s_scale = torch.tensor(smoke_scale.copy()).expand([3]).float()\n",
    "        self.s_min = torch.Tensor(in_min)\n",
    "        self.s_max = torch.Tensor(in_max)\n",
    "\n",
    "    def world2sim(self, pts_world):\n",
    "        pts_world_homo = torch.cat([pts_world, torch.ones_like(pts_world[..., :1])], dim=-1)\n",
    "        pts_sim_ = torch.matmul(self.s_w2s, pts_world_homo[..., None]).squeeze(-1)[..., :3]\n",
    "        pts_sim = pts_sim_ / (self.s_scale)  # 3.target to 2.simulation\n",
    "        return pts_sim\n",
    "\n",
    "    def world2sim_rot(self, pts_world):\n",
    "        pts_sim_ = torch.matmul(self.s_w2s[:3, :3], pts_world[..., None]).squeeze(-1)\n",
    "        pts_sim = pts_sim_ / (self.s_scale)  # 3.target to 2.simulation\n",
    "        return pts_sim\n",
    "\n",
    "    def sim2world(self, pts_sim):\n",
    "        pts_sim_ = pts_sim * self.s_scale\n",
    "        pts_sim_homo = torch.cat([pts_sim_, torch.ones_like(pts_sim_[..., :1])], dim=-1)\n",
    "        pts_world = torch.matmul(self.s2w, pts_sim_homo[..., None]).squeeze(-1)[..., :3]\n",
    "        return pts_world\n",
    "\n",
    "    def sim2world_rot(self, pts_sim):\n",
    "        pts_sim_ = pts_sim * self.s_scale\n",
    "        pts_world = torch.matmul(self.s2w[:3, :3], pts_sim_[..., None]).squeeze(-1)\n",
    "        return pts_world\n",
    "\n",
    "    def isInside(self, inputs_pts):\n",
    "        target_pts = pos_world2smoke(inputs_pts, self.s_w2s, self.s_scale)\n",
    "        above = torch.logical_and(target_pts[..., 0] >= self.s_min[0], target_pts[..., 1] >= self.s_min[1])\n",
    "        above = torch.logical_and(above, target_pts[..., 2] >= self.s_min[2])\n",
    "        below = torch.logical_and(target_pts[..., 0] <= self.s_max[0], target_pts[..., 1] <= self.s_max[1])\n",
    "        below = torch.logical_and(below, target_pts[..., 2] <= self.s_max[2])\n",
    "        outputs = torch.logical_and(below, above)\n",
    "        return outputs\n",
    "\n",
    "    def insideMask(self, inputs_pts, to_float=True):\n",
    "        return self.isInside(inputs_pts).to(torch.float) if to_float else self.isInside(inputs_pts)\n",
    "\n",
    "\n",
    "voxel_tran_inv = np.linalg.inv(voxel_tran)\n",
    "bbox_model = BBox_Tool(voxel_tran_inv, voxel_scale)"
   ],
   "id": "9486a5025ef44e41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@ti.func\n",
    "def linear_step(t):\n",
    "    return t\n",
    "\n",
    "\n",
    "@ti.func\n",
    "def d_linear_step(t):\n",
    "    return 1\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def torch2ti(field: ti.template(), data: ti.types.ndarray()):\n",
    "    for I in ti.grouped(data):\n",
    "        field[I] = data[I]\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def ti2torch(field: ti.template(), data: ti.types.ndarray()):\n",
    "    for I in ti.grouped(data):\n",
    "        data[I] = field[I]\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def ti2torch_grad(field: ti.template(), grad: ti.types.ndarray()):\n",
    "    for I in ti.grouped(grad):\n",
    "        grad[I] = field.grad[I]\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def torch2ti_grad(field: ti.template(), grad: ti.types.ndarray()):\n",
    "    for I in ti.grouped(grad):\n",
    "        field.grad[I] = grad[I]\n",
    "\n",
    "\n",
    "@ti.func\n",
    "def fast_hash(pos_grid_local):\n",
    "    result = ti.uint32(0)\n",
    "    primes = ti.math.uvec4(ti.uint32(1), ti.uint32(2654435761), ti.uint32(805459861), ti.uint32(3674653429))\n",
    "    for i in ti.static(range(4)):\n",
    "        result ^= ti.uint32(pos_grid_local[i]) * primes[i]\n",
    "    return result\n",
    "\n",
    "\n",
    "# ravel (i, j, k, t) to i + i_dim * j + (i_dim * j_dim) * k + (i_dim * j_dim * k_dim) * t\n",
    "@ti.func\n",
    "def under_hash(pos_grid_local, resolution):\n",
    "    result = ti.uint32(0)\n",
    "    stride = ti.uint32(1)\n",
    "    for i in ti.static(range(4)):\n",
    "        result += ti.uint32(pos_grid_local[i] * stride)\n",
    "        stride *= resolution[i] + 1  # note the +1 here, because 256 x 256 grid actually has 257 x 257 entries\n",
    "    return result\n",
    "\n",
    "\n",
    "@ti.func\n",
    "def grid_pos2hash_index(indicator, pos_grid_local, plane_res, map_size):\n",
    "    hash_result = ti.uint32(0)\n",
    "    if indicator == 1:\n",
    "        hash_result = under_hash(pos_grid_local, plane_res)\n",
    "    else:\n",
    "        hash_result = fast_hash(pos_grid_local)\n",
    "\n",
    "    return hash_result % map_size\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def hash_encode_kernel(\n",
    "        xyzts: ti.template(),\n",
    "        table: ti.template(),\n",
    "        xyzts_embedding: ti.template(),\n",
    "        hash_map_indicator: ti.template(),\n",
    "        hash_map_sizes_field: ti.template(),\n",
    "        hash_map_shapes_field: ti.template(),\n",
    "        offsets: ti.template(),\n",
    "        B: ti.i32,\n",
    "        num_scales: ti.i32):\n",
    "    ti.loop_config(block_dim=16)\n",
    "    for i, level in ti.ndrange(B, num_scales):\n",
    "        res_x = hash_map_shapes_field[level, 0]\n",
    "        res_y = hash_map_shapes_field[level, 1]\n",
    "        res_z = hash_map_shapes_field[level, 2]\n",
    "        res_t = hash_map_shapes_field[level, 3]\n",
    "        plane_res = ti.Vector([res_x, res_y, res_z, res_t])\n",
    "        pos = ti.Vector([xyzts[i, 0], xyzts[i, 1], xyzts[i, 2], xyzts[i, 3]]) * plane_res\n",
    "\n",
    "        pos_grid_uint = ti.cast(ti.floor(pos), ti.uint32)  # floor\n",
    "        pos_grid_uint = ti.math.clamp(pos_grid_uint, 0, plane_res - 1)\n",
    "        pos -= pos_grid_uint  # pos now represents frac\n",
    "        pos = ti.math.clamp(pos, 0.0, 1.0)\n",
    "\n",
    "        offset = offsets[level]\n",
    "\n",
    "        indicator = hash_map_indicator[level]\n",
    "        map_size = hash_map_sizes_field[level]\n",
    "\n",
    "        local_feature_0 = 0.0\n",
    "        local_feature_1 = 0.0\n",
    "\n",
    "        for idx in ti.static(range(16)):\n",
    "            w = 1.\n",
    "            pos_grid_local = ti.math.uvec4(0)\n",
    "\n",
    "            for d in ti.static(range(4)):\n",
    "                t = linear_step(pos[d])\n",
    "                if (idx & (1 << d)) == 0:\n",
    "                    pos_grid_local[d] = pos_grid_uint[d]\n",
    "                    w *= 1 - t\n",
    "                else:\n",
    "                    pos_grid_local[d] = pos_grid_uint[d] + 1\n",
    "                    w *= t\n",
    "\n",
    "            index = grid_pos2hash_index(indicator, pos_grid_local, plane_res, map_size)\n",
    "            index_table = offset + index * 2  # the flat index for the 1st entry\n",
    "            index_table_int = ti.cast(index_table, ti.int32)\n",
    "            local_feature_0 += w * table[index_table_int]\n",
    "            local_feature_1 += w * table[index_table_int + 1]\n",
    "\n",
    "        xyzts_embedding[i, level * 2] = local_feature_0\n",
    "        xyzts_embedding[i, level * 2 + 1] = local_feature_1\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def hash_encode_kernel_grad(\n",
    "        xyzts: ti.template(),\n",
    "        table: ti.template(),\n",
    "        xyzts_embedding: ti.template(),\n",
    "        hash_map_indicator: ti.template(),\n",
    "        hash_map_sizes_field: ti.template(),\n",
    "        hash_map_shapes_field: ti.template(),\n",
    "        offsets: ti.template(),\n",
    "        B: ti.i32,\n",
    "        num_scales: ti.i32,\n",
    "        xyzts_grad: ti.template(),\n",
    "        table_grad: ti.template(),\n",
    "        output_grad: ti.template()):\n",
    "    # # # get hash table embedding\n",
    "\n",
    "    ti.loop_config(block_dim=16)\n",
    "    for i, level in ti.ndrange(B, num_scales):\n",
    "        res_x = hash_map_shapes_field[level, 0]\n",
    "        res_y = hash_map_shapes_field[level, 1]\n",
    "        res_z = hash_map_shapes_field[level, 2]\n",
    "        res_t = hash_map_shapes_field[level, 3]\n",
    "        plane_res = ti.Vector([res_x, res_y, res_z, res_t])\n",
    "        pos = ti.Vector([xyzts[i, 0], xyzts[i, 1], xyzts[i, 2], xyzts[i, 3]]) * plane_res\n",
    "\n",
    "        pos_grid_uint = ti.cast(ti.floor(pos), ti.uint32)  # floor\n",
    "        pos_grid_uint = ti.math.clamp(pos_grid_uint, 0, plane_res - 1)\n",
    "        pos -= pos_grid_uint  # pos now represents frac\n",
    "        pos = ti.math.clamp(pos, 0.0, 1.0)\n",
    "\n",
    "        offset = offsets[level]\n",
    "\n",
    "        indicator = hash_map_indicator[level]\n",
    "        map_size = hash_map_sizes_field[level]\n",
    "\n",
    "        for idx in ti.static(range(16)):\n",
    "            w = 1.\n",
    "            pos_grid_local = ti.math.uvec4(0)\n",
    "            dw = ti.Vector([0., 0., 0., 0.])\n",
    "            # prods = ti.Vector([0., 0., 0.,0.])\n",
    "            for d in ti.static(range(4)):\n",
    "                t = linear_step(pos[d])\n",
    "                dt = d_linear_step(pos[d])\n",
    "                if (idx & (1 << d)) == 0:\n",
    "                    pos_grid_local[d] = pos_grid_uint[d]\n",
    "                    w *= 1 - t\n",
    "                    dw[d] = -dt\n",
    "\n",
    "                else:\n",
    "                    pos_grid_local[d] = pos_grid_uint[d] + 1\n",
    "                    w *= t\n",
    "                    dw[d] = dt\n",
    "\n",
    "            index = grid_pos2hash_index(indicator, pos_grid_local, plane_res, map_size)\n",
    "            index_table = offset + index * 2  # the flat index for the 1st entry\n",
    "            index_table_int = ti.cast(index_table, ti.int32)\n",
    "            table_grad[index_table_int] += w * output_grad[i, 2 * level]\n",
    "            table_grad[index_table_int + 1] += w * output_grad[i, 2 * level + 1]\n",
    "            for d in ti.static(range(4)):\n",
    "                # eps = 1e-15\n",
    "                # prod = w / ((linear_step(pos[d]) if idx & (1 << d) > 0 else 1 - linear_step(pos[d])) + eps)\n",
    "                # prod=1.0\n",
    "                # for k in range(4):\n",
    "                #     if k == d:\n",
    "                #         prod *= dw[k]\n",
    "                #     else:\n",
    "                #         prod *= 1- linear_step(pos[k]) if (idx & (1 << k) == 0) else linear_step(pos[k])\n",
    "                prod = dw[d] * (\n",
    "                    linear_step(pos[(d + 1) % 4]) if (idx & (1 << ((d + 1) % 4)) > 0) else 1 - linear_step(\n",
    "                        pos[(d + 1) % 4])\n",
    "                ) * (\n",
    "                           linear_step(pos[(d + 2) % 4]) if (idx & (1 << ((d + 2) % 4)) > 0) else 1 - linear_step(\n",
    "                               pos[(d + 2) % 4])\n",
    "                       ) * (\n",
    "                           linear_step(pos[(d + 3) % 4]) if (idx & (1 << ((d + 3) % 4)) > 0) else 1 - linear_step(\n",
    "                               pos[(d + 3) % 4])\n",
    "                       )\n",
    "                xyzts_grad[i, d] += table[index_table_int] * prod * plane_res[d] * output_grad[i, 2 * level]\n",
    "                xyzts_grad[i, d] += table[index_table_int + 1] * prod * plane_res[d] * output_grad[i, 2 * level + 1]\n",
    "\n",
    "\n",
    "class HashEncoderHyFluid(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            min_res: np.array,\n",
    "            max_res: np.array,\n",
    "            num_scales: int,\n",
    "            max_params=2 ** 19,\n",
    "            features_per_level: int = 2,\n",
    "            max_num_queries=10000000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        b = np.exp((np.log(max_res) - np.log(min_res)) / (num_scales - 1))\n",
    "\n",
    "        hash_map_shapes = []\n",
    "        hash_map_sizes = []\n",
    "        hash_map_indicator = []\n",
    "        offsets = []\n",
    "        total_hash_size = 0\n",
    "        for scale_i in range(num_scales):\n",
    "            res = np.ceil(min_res * np.power(b, scale_i)).astype(int)\n",
    "            params_in_level_raw = np.int64(res[0] + 1) * np.int64(res[1] + 1) * np.int64(res[2] + 1) * np.int64(\n",
    "                res[3] + 1)\n",
    "            params_in_level = int(params_in_level_raw) if params_in_level_raw % 8 == 0 else int(\n",
    "                (params_in_level_raw + 8 - 1) / 8) * 8\n",
    "            params_in_level = min(max_params, params_in_level)\n",
    "            hash_map_shapes.append(res)\n",
    "            hash_map_sizes.append(params_in_level)\n",
    "            hash_map_indicator.append(1 if params_in_level_raw <= params_in_level else 0)\n",
    "            offsets.append(total_hash_size)\n",
    "            total_hash_size += params_in_level * features_per_level\n",
    "\n",
    "        ####################################################################################################\n",
    "        self.hash_map_shapes_field = ti.field(dtype=ti.i32, shape=(num_scales, 4))\n",
    "        self.hash_map_shapes_field.from_numpy(np.array(hash_map_shapes))\n",
    "\n",
    "        self.hash_map_sizes_field = ti.field(dtype=ti.i32, shape=(num_scales,))\n",
    "        self.hash_map_sizes_field.from_numpy(np.array(hash_map_sizes))\n",
    "\n",
    "        self.hash_map_indicator_field = ti.field(dtype=ti.i32, shape=(num_scales,))\n",
    "        self.hash_map_indicator_field.from_numpy(np.array(hash_map_indicator))\n",
    "\n",
    "        self.offsets_fields = ti.field(ti.i32, shape=(num_scales,))\n",
    "        self.offsets_fields.from_numpy(np.array(offsets))\n",
    "\n",
    "        self.hash_table = torch.nn.Parameter(\n",
    "            (torch.rand(size=(total_hash_size,), dtype=torch.float32) * 2.0 - 1.0) * 1e-4, requires_grad=True)\n",
    "\n",
    "        self.parameter_fields = ti.field(dtype=ti.f32, shape=(total_hash_size,), needs_grad=True)\n",
    "        self.parameter_fields_grad = ti.field(dtype=ti.f32, shape=(total_hash_size,), needs_grad=True)\n",
    "\n",
    "        self.output_fields = ti.field(dtype=ti.f32, shape=(max_num_queries, num_scales * features_per_level),\n",
    "                                      needs_grad=True)\n",
    "        self.output_grad = ti.field(dtype=ti.f32, shape=(max_num_queries, num_scales * features_per_level),\n",
    "                                    needs_grad=True)\n",
    "\n",
    "        self.input_fields = ti.field(dtype=ti.f32, shape=(max_num_queries, 4), needs_grad=True)\n",
    "        self.input_fields_grad = ti.field(dtype=ti.f32, shape=(max_num_queries, 4), needs_grad=True)\n",
    "\n",
    "        self.num_scales = num_scales\n",
    "        self.features_per_level = features_per_level\n",
    "        ####################################################################################################\n",
    "\n",
    "        self.register_buffer('hash_grad', torch.zeros(total_hash_size, dtype=torch.float32), persistent=False)\n",
    "        self.register_buffer('hash_grad2', torch.zeros(total_hash_size, dtype=torch.float32), persistent=False)\n",
    "        self.register_buffer('input_grad', torch.zeros(max_num_queries, 4, dtype=torch.float32), persistent=False)\n",
    "        self.register_buffer('input_grad2', torch.zeros(max_num_queries, 4, dtype=torch.float32), persistent=False)\n",
    "        self.register_buffer('output_embedding', torch.zeros(max_num_queries, num_scales * 2, dtype=torch.float32),\n",
    "                             persistent=False)\n",
    "\n",
    "        ####################################################################################################\n",
    "        class ModuleFunction(torch.autograd.Function):\n",
    "            @staticmethod\n",
    "            @torch.amp.custom_fwd(cast_inputs=torch.float32,\n",
    "                                  device_type='cuda')  # @custom_fwd(cast_inputs=torch.float32)\n",
    "            def forward(ctx, input_pos, params):\n",
    "                output_embedding = self.output_embedding[:input_pos.shape[0]].contiguous()\n",
    "                torch2ti(self.input_fields, input_pos.contiguous())\n",
    "                torch2ti(self.parameter_fields, params.contiguous())\n",
    "\n",
    "                hash_encode_kernel(\n",
    "                    self.input_fields,\n",
    "                    self.parameter_fields,\n",
    "                    self.output_fields,\n",
    "                    self.hash_map_indicator_field,\n",
    "                    self.hash_map_sizes_field,\n",
    "                    self.hash_map_shapes_field,\n",
    "                    self.offsets_fields,\n",
    "                    input_pos.shape[0],\n",
    "                    self.num_scales,\n",
    "                )\n",
    "\n",
    "                ti2torch(self.output_fields, output_embedding)\n",
    "                ctx.save_for_backward(input_pos, params)\n",
    "                return output_embedding\n",
    "\n",
    "            @staticmethod\n",
    "            @torch.amp.custom_bwd(device_type='cuda')  # @custom_bwd\n",
    "            def backward(ctx, doutput):\n",
    "                self.input_fields.grad.fill(0.)\n",
    "                self.input_fields_grad.fill(0.)\n",
    "                self.parameter_fields.grad.fill(0.)\n",
    "                self.parameter_fields_grad.fill(0.)\n",
    "\n",
    "                input_pos, params = ctx.saved_tensors\n",
    "                return self.module_function_grad.apply(input_pos, params, doutput)\n",
    "\n",
    "        class ModuleFunctionGrad(torch.autograd.Function):\n",
    "            @staticmethod\n",
    "            @torch.amp.custom_fwd(cast_inputs=torch.float32,\n",
    "                                  device_type='cuda')  # @custom_fwd(cast_inputs=torch.float32)\n",
    "            def forward(ctx, input_pos, params, doutput):\n",
    "                torch2ti(self.input_fields, input_pos.contiguous())\n",
    "                torch2ti(self.parameter_fields, params.contiguous())\n",
    "                torch2ti(self.output_grad, doutput.contiguous())\n",
    "\n",
    "                hash_encode_kernel_grad(\n",
    "                    self.input_fields,\n",
    "                    self.parameter_fields,\n",
    "                    self.output_fields,\n",
    "                    self.hash_map_indicator_field,\n",
    "                    self.hash_map_sizes_field,\n",
    "                    self.hash_map_shapes_field,\n",
    "                    self.offsets_fields,\n",
    "                    doutput.shape[0],\n",
    "                    self.num_scales,\n",
    "                    self.input_fields_grad,\n",
    "                    self.parameter_fields_grad,\n",
    "                    self.output_grad\n",
    "                )\n",
    "\n",
    "                ti2torch(self.input_fields_grad, self.input_grad.contiguous())\n",
    "                ti2torch(self.parameter_fields_grad, self.hash_grad.contiguous())\n",
    "                return self.input_grad[:doutput.shape[0]], self.hash_grad\n",
    "\n",
    "            @staticmethod\n",
    "            @torch.amp.custom_bwd(device_type='cuda')  # @custom_bwd\n",
    "            def backward(ctx, d_input_grad, d_hash_grad):\n",
    "                self.parameter_fields.grad.fill(0.)\n",
    "                self.input_fields.grad.fill(0.)\n",
    "                torch2ti_grad(self.input_fields_grad, d_input_grad.contiguous())\n",
    "                torch2ti_grad(self.parameter_fields_grad, d_hash_grad.contiguous())\n",
    "\n",
    "                hash_encode_kernel_grad.grad(\n",
    "                    self.input_fields,\n",
    "                    self.parameter_fields,\n",
    "                    self.output_fields,\n",
    "                    self.hash_map_indicator_field,\n",
    "                    self.hash_map_sizes_field,\n",
    "                    self.hash_map_shapes_field,\n",
    "                    self.offsets_fields,\n",
    "                    d_input_grad.shape[0],\n",
    "                    self.num_scales,\n",
    "                    self.input_fields_grad,\n",
    "                    self.parameter_fields_grad,\n",
    "                    self.output_grad\n",
    "                )\n",
    "\n",
    "                ti2torch_grad(self.input_fields, self.input_grad2.contiguous()[:d_input_grad.shape[0]])\n",
    "                ti2torch_grad(self.parameter_fields, self.hash_grad2.contiguous())\n",
    "                # set_trace(term_size=(120,30))\n",
    "                return self.input_grad2[:d_input_grad.shape[0]], self.hash_grad2, None\n",
    "\n",
    "        self.module_function = ModuleFunction\n",
    "        self.module_function_grad = ModuleFunctionGrad\n",
    "        ####################################################################################################\n",
    "\n",
    "    def forward(self, positions):\n",
    "        # positions: (N, 4), normalized to [-1, 1]\n",
    "        positions = positions * 0.5 + 0.5\n",
    "        return self.module_function.apply(positions, self.hash_table)\n",
    "\n",
    "\n",
    "ti.init(ti.cuda)\n",
    "max_res = np.array([256, 256, 256, 128])\n",
    "min_res = np.array([16, 16, 16, 16])\n",
    "embed_fn = HashEncoderHyFluid(min_res=min_res, max_res=max_res, num_scales=16, max_params=2 ** 19)\n",
    "input_ch = embed_fn.num_scales * 2\n",
    "embedding_params = list(embed_fn.parameters())\n",
    "print(f'embedding_params: {len(embedding_params)}, {embedding_params[0].shape}')"
   ],
   "id": "8f785993eaa9d6eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Small NeRF for Hash embeddings\n",
    "class NeRFSmall(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers=3,\n",
    "                 hidden_dim=64,\n",
    "                 geo_feat_dim=15,\n",
    "                 num_layers_color=2,\n",
    "                 hidden_dim_color=16,\n",
    "                 input_ch=3,\n",
    "                 ):\n",
    "        super(NeRFSmall, self).__init__()\n",
    "\n",
    "        self.input_ch = input_ch\n",
    "        self.rgb = torch.nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        # sigma network\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.geo_feat_dim = geo_feat_dim\n",
    "\n",
    "        sigma_net = []\n",
    "        for l in range(num_layers):\n",
    "            if l == 0:\n",
    "                in_dim = self.input_ch\n",
    "            else:\n",
    "                in_dim = hidden_dim\n",
    "\n",
    "            if l == num_layers - 1:\n",
    "                out_dim = 1  # 1 sigma + 15 SH features for color\n",
    "            else:\n",
    "                out_dim = hidden_dim\n",
    "\n",
    "            sigma_net.append(torch.nn.Linear(in_dim, out_dim, bias=False))\n",
    "\n",
    "        self.sigma_net = torch.nn.ModuleList(sigma_net)\n",
    "\n",
    "        self.color_net = []\n",
    "        for l in range(num_layers_color):\n",
    "            if l == 0:\n",
    "                in_dim = 1\n",
    "            else:\n",
    "                in_dim = hidden_dim_color\n",
    "\n",
    "            if l == num_layers_color - 1:\n",
    "                out_dim = 1\n",
    "            else:\n",
    "                out_dim = hidden_dim_color\n",
    "\n",
    "            self.color_net.append(torch.nn.Linear(in_dim, out_dim, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.sigma_net[l](h)\n",
    "            h = torch.nn.functional.relu(h, inplace=True)\n",
    "\n",
    "        sigma = h\n",
    "        return sigma\n",
    "\n",
    "\n",
    "model = NeRFSmall(num_layers=2,\n",
    "                  hidden_dim=64,\n",
    "                  geo_feat_dim=15,\n",
    "                  num_layers_color=2,\n",
    "                  hidden_dim_color=16,\n",
    "                  input_ch=input_ch).to(device)\n",
    "grad_vars = list(model.parameters())\n",
    "print(f'grad_vars: {len(grad_vars)}, {grad_vars[0].shape}, {grad_vars[1].shape}, {grad_vars[2].shape}')\n",
    "\n",
    "network_query_fn = lambda x: model(embed_fn(x))"
   ],
   "id": "40c30fa37a97cfab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "\n",
    "        self.degenerated_to_sgd = degenerated_to_sgd\n",
    "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
    "            for param in params:\n",
    "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
    "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
    "                        buffer=[[None, None, None] for _ in range(10)])\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = group['buffer'][int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt(\n",
    "                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
    "                                    N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    elif self.degenerated_to_sgd:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = -1\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(p_data_fp32, alpha=-group['weight_decay'] * group['lr'])\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "                elif step_size > 0:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(p_data_fp32, alpha=-group['weight_decay'] * group['lr'])\n",
    "                    p_data_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "optimizer = RAdam([\n",
    "    {'params': grad_vars, 'weight_decay': 1e-6},\n",
    "    {'params': embedding_params, 'eps': 1e-15}\n",
    "], lr=0.01, betas=(0.9, 0.99))\n",
    "grad_vars += list(embedding_params)"
   ],
   "id": "4a8c7b72aefdded8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Device: {torch.cuda.get_device_name(current_device)}\")\n",
    "print(f\"Allocated Memory: {torch.cuda.memory_allocated(current_device) / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Cached Memory: {torch.cuda.memory_reserved(current_device) / 1024 ** 2:.2f} MB\")"
   ],
   "id": "9a2c26944faa6da4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "render_poses = torch.tensor(render_poses, device=device)",
   "id": "a2864465ca4f504d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Device: {torch.cuda.get_device_name(current_device)}\")\n",
    "print(f\"Allocated Memory: {torch.cuda.memory_allocated(current_device) / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Cached Memory: {torch.cuda.memory_reserved(current_device) / 1024 ** 2:.2f} MB\")"
   ],
   "id": "d785f96b9cbc08d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_rays_np_continuous(H, W, K, c2w):\n",
    "    i, j = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy')\n",
    "    random_offset_i = np.random.uniform(0, 1, size=(H, W))\n",
    "    random_offset_j = np.random.uniform(0, 1, size=(H, W))\n",
    "    i = i + random_offset_i\n",
    "    j = j + random_offset_j\n",
    "    i = np.clip(i, 0, W - 1)\n",
    "    j = np.clip(j, 0, H - 1)\n",
    "\n",
    "    dirs = np.stack([(i - K[0][2]) / K[0][0], -(j - K[1][2]) / K[1][1], -np.ones_like(i)], -1)\n",
    "    # Rotate ray directions from camera frame to the world frame\n",
    "    rays_d = np.sum(dirs[..., np.newaxis, :] * c2w[:3, :3],\n",
    "                    -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n",
    "    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n",
    "    rays_o = np.broadcast_to(c2w[:3, -1], np.shape(rays_d))\n",
    "    return rays_o, rays_d, i, j\n",
    "\n",
    "\n",
    "def sample_bilinear(img, xy):\n",
    "    \"\"\"\n",
    "    Sample image with bilinear interpolation\n",
    "    :param img: (T, V, H, W, 3)\n",
    "    :param xy: (V, 2, H, W)\n",
    "    :return: img: (T, V, H, W, 3)\n",
    "    \"\"\"\n",
    "    T, V, H, W, _ = img.shape\n",
    "    u, v = xy[:, 0], xy[:, 1]\n",
    "\n",
    "    u = np.clip(u, 0, W - 1)\n",
    "    v = np.clip(v, 0, H - 1)\n",
    "\n",
    "    u_floor, v_floor = np.floor(u).astype(int), np.floor(v).astype(int)\n",
    "    u_ceil, v_ceil = np.ceil(u).astype(int), np.ceil(v).astype(int)\n",
    "\n",
    "    u_ratio, v_ratio = u - u_floor, v - v_floor\n",
    "    u_ratio, v_ratio = u_ratio[None, ..., None], v_ratio[None, ..., None]\n",
    "\n",
    "    bottom_left = img[:, np.arange(V)[:, None, None], v_floor, u_floor]\n",
    "    bottom_right = img[:, np.arange(V)[:, None, None], v_floor, u_ceil]\n",
    "    top_left = img[:, np.arange(V)[:, None, None], v_ceil, u_floor]\n",
    "    top_right = img[:, np.arange(V)[:, None, None], v_ceil, u_ceil]\n",
    "\n",
    "    bottom = (1 - u_ratio) * bottom_left + u_ratio * bottom_right\n",
    "    top = (1 - u_ratio) * top_left + u_ratio * top_right\n",
    "\n",
    "    interpolated = (1 - v_ratio) * bottom + v_ratio * top\n",
    "\n",
    "    return interpolated\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "H, W, focal = hwf\n",
    "H, W = int(H), int(W)\n",
    "hwf = [H, W, focal]\n",
    "K = np.array([\n",
    "    [focal, 0, 0.5 * W],\n",
    "    [0, focal, 0.5 * H],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "# anti-aliasing\n",
    "rays = []\n",
    "ij = []\n",
    "for p in poses_train[:, :3, :4]:\n",
    "    r_o, r_d, i_, j_ = get_rays_np_continuous(H, W, K, p)\n",
    "    rays.append([r_o, r_d])\n",
    "    ij.append([i_, j_])\n",
    "rays = np.stack(rays, 0)  # [V, ro+rd=2, H, W, 3]\n",
    "ij = np.stack(ij, 0)  # [V, 2, H, W]\n",
    "print(f'Elapsed time: {time.time() - start:.2f} s')\n",
    "start = time.time()\n",
    "images_train = sample_bilinear(images_train_, ij)  # [T, V, H, W, 3]\n",
    "print(f'Elapsed time: {time.time() - start:.2f} s')\n",
    "rays = np.transpose(rays, [0, 2, 3, 1, 4])  # [V, H, W, ro+rd=2, 3]\n",
    "rays = np.reshape(rays, [-1, 2, 3])  # [VHW, ro+rd=2, 3]\n",
    "rays = rays.astype(np.float32)\n",
    "\n",
    "memory_rays = rays.nbytes / (1024 ** 2)  # 转换为 MB\n",
    "memory_images_train = images_train.nbytes / (1024 ** 2)  # 转换为 MB\n",
    "print(f'rays: {memory_rays:.2f} MB, shape: {rays.shape}')\n",
    "print(f'images_train: {memory_images_train:.2f} MB, shape: {images_train.shape}')"
   ],
   "id": "71a1196b8b098699",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "images_train = torch.tensor(images_train, device=device).flatten(start_dim=1, end_dim=3)  # [T, VHW, 3]\n",
    "T, S, _ = images_train.shape\n",
    "rays = torch.tensor(rays, device=device)\n",
    "ray_idxs = torch.randperm(rays.shape[0])"
   ],
   "id": "51aa758db9946ea1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Device: {torch.cuda.get_device_name(current_device)}\")\n",
    "print(f\"Allocated Memory: {torch.cuda.memory_allocated(current_device) / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Cached Memory: {torch.cuda.memory_reserved(current_device) / 1024 ** 2:.2f} MB\")"
   ],
   "id": "faea5b22a1b72777",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    val = 0\n",
    "    avg = 0\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    tot_count = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.tot_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = float(val)\n",
    "        self.sum += float(val) * n\n",
    "        self.count += n\n",
    "        self.tot_count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "i_batch = 0\n",
    "start = 1\n",
    "global_step = start\n",
    "loss_meter, psnr_meter = AverageMeter(), AverageMeter()\n",
    "loss_list = []\n",
    "psnr_list = []\n",
    "resample_rays = False\n",
    "N_rand = 256\n",
    "N_time = 1\n",
    "N_samples = 192\n",
    "\n",
    "img2mse = lambda x, y: torch.mean((x - y) ** 2)\n",
    "mse2psnr = lambda x: -10. * torch.log(x) / torch.log(torch.Tensor([10.]))\n",
    "to8b = lambda x: (255 * np.clip(x, 0, 1)).astype(np.uint8)\n",
    "lrate = 0.01\n",
    "lrate_decay = 100000\n",
    "\n",
    "basedir = './logs'\n",
    "expname = 'exp_real/density_256_128'\n",
    "os.makedirs(os.path.join(basedir, expname), exist_ok=True)"
   ],
   "id": "845960b9947bc68c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_rays(directions, c2w):\n",
    "    \"\"\"\n",
    "    Get ray origin and normalized directions in world coordinate for all pixels in one image.\n",
    "    Reference: https://www.scratchapixel.com/lessons/3d-basic-rendering/\n",
    "               ray-tracing-generating-camera-rays/standard-coordinate-systems\n",
    "\n",
    "    Inputs:\n",
    "        directions: (H, W, 3) precomputed ray directions in camera coordinate\n",
    "        c2w: (3, 4) transformation matrix from camera coordinate to world coordinate\n",
    "\n",
    "    Outputs:\n",
    "        rays_o: (H*W, 3), the origin of the rays in world coordinate\n",
    "        rays_d: (H*W, 3), the normalized direction of the rays in world coordinate\n",
    "    \"\"\"\n",
    "    # Rotate ray directions from camera coordinate to the world coordinate\n",
    "    rays_d = directions @ c2w[:3, :3].T  # (H, W, 3)\n",
    "    rays_d = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "    # The origin of all rays is the camera origin in world coordinate\n",
    "    rays_o = c2w[:3, -1].expand(rays_d.shape)  # (H, W, 3)\n",
    "\n",
    "    rays_d = rays_d.view(-1, 3)\n",
    "    rays_o = rays_o.view(-1, 3)\n",
    "\n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n",
    "def raw2outputs(raw, z_vals, rays_d):\n",
    "    \"\"\"Transforms model's predictions to semantically meaningful values.\n",
    "    Args:\n",
    "        raw: [num_rays, num_samples along ray, 4]. Prediction from model.\n",
    "        z_vals: [num_rays, num_samples along ray]. Integration time.\n",
    "        rays_d: [num_rays, 3]. Direction of each ray.\n",
    "    Returns:\n",
    "        rgb_map: [num_rays, 3]. Estimated RGB color of a ray.\n",
    "        disp_map: [num_rays]. Disparity map. Inverse of depth map.\n",
    "        acc_map: [num_rays]. Sum of weights along each ray.\n",
    "        weights: [num_rays, num_samples]. Weights assigned to each sampled color.\n",
    "        depth_map: [num_rays]. Estimated distance to object.\n",
    "    \"\"\"\n",
    "    raw2alpha = lambda raw, dists, act_fn=torch.nn.functional.relu: 1. - torch.exp(-act_fn(raw) * dists)\n",
    "\n",
    "    dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "    dists = torch.cat([dists, torch.Tensor([1e10]).expand(dists[..., :1].shape)], -1)  # [N_rays, N_samples]\n",
    "\n",
    "    dists = dists * torch.norm(rays_d[..., None, :], dim=-1)\n",
    "\n",
    "    rgb = torch.ones(3) * (0.6 + torch.tanh(model.rgb) * 0.4)\n",
    "    # rgb = 0.6 + torch.tanh(learned_rgb) * 0.4\n",
    "    noise = 0.\n",
    "\n",
    "    alpha = raw2alpha(raw[..., -1] + noise, dists)  # [N_rays, N_samples]\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)), 1. - alpha + 1e-10], -1), -1)[:,\n",
    "                      :-1]  # [N_rays, N_samples]\n",
    "    rgb_map = torch.sum(weights[..., None] * rgb, -2)  # [N_rays, 3]\n",
    "\n",
    "    depth_map = torch.sum(weights * z_vals, -1) / (torch.sum(weights, -1) + 1e-10)\n",
    "    disp_map = 1. / torch.max(1e-10 * torch.ones_like(depth_map), depth_map)\n",
    "    acc_map = torch.sum(weights, -1)\n",
    "    depth_map[acc_map < 1e-1] = 0.\n",
    "\n",
    "    return rgb_map, disp_map, acc_map, weights, depth_map\n",
    "\n",
    "\n",
    "def render_rays(ray_batch,\n",
    "                retraw=False,\n",
    "                perturb=0.):\n",
    "    \"\"\"Volumetric rendering.\n",
    "    Args:\n",
    "      ray_batch: array of shape [batch_size, ...]. All information necessary\n",
    "        for sampling along a ray, including: ray origin, ray direction, min\n",
    "        dist, max dist, and unit-magnitude viewing direction.\n",
    "      perturb: float, 0 or 1. If non-zero, each ray is sampled at stratified\n",
    "        random points in time.\n",
    "    Returns:\n",
    "      rgb_map: [num_rays, 3]. Estimated RGB color of a ray.\n",
    "      disp_map: [num_rays]. Disparity map. 1 / depth.\n",
    "      acc_map: [num_rays]. Accumulated opacity along each ray.\n",
    "      raw: [num_rays, num_samples, 4]. Raw predictions from model.\n",
    "      z_std: [num_rays]. Standard deviation of distances along ray for each\n",
    "        sample.\n",
    "    \"\"\"\n",
    "    N_rays = ray_batch.shape[0]\n",
    "    rays_o, rays_d = ray_batch[:, 0:3], ray_batch[:, 3:6]  # [N_rays, 3] each\n",
    "    time_step = ray_batch[:, -1]\n",
    "    bounds = torch.reshape(ray_batch[..., 6:8], [-1, 1, 2])\n",
    "    near, far = bounds[..., 0], bounds[..., 1]  # [-1,1]\n",
    "\n",
    "    t_vals = torch.linspace(0., 1., steps=N_samples)\n",
    "    z_vals = near * (1. - t_vals) + far * (t_vals)\n",
    "\n",
    "    z_vals = z_vals.expand([N_rays, N_samples])\n",
    "\n",
    "    if perturb > 0.:\n",
    "        # get intervals between samples\n",
    "        mids = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "        upper = torch.cat([mids, z_vals[..., -1:]], -1)\n",
    "        lower = torch.cat([z_vals[..., :1], mids], -1)\n",
    "        # stratified samples in those intervals\n",
    "        t_rand = torch.rand(z_vals.shape)\n",
    "\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "\n",
    "    pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]  # [N_rays, N_samples, 3]\n",
    "    pts_time_step = time_step[..., None, None].expand(-1, pts.shape[1], -1)\n",
    "    pts = torch.cat([pts, pts_time_step], -1)  # [..., 4]\n",
    "    pts_flat = torch.reshape(pts, [-1, 4])\n",
    "    out_dim = 1\n",
    "    raw_flat = torch.zeros([N_rays, N_samples, out_dim]).reshape(-1, out_dim)\n",
    "\n",
    "    bbox_mask = bbox_model.insideMask(pts_flat[..., :3], to_float=False)\n",
    "    if bbox_mask.sum() == 0:\n",
    "        bbox_mask[0] = True  # in case zero rays are inside the bbox\n",
    "    pts = pts_flat[bbox_mask]\n",
    "\n",
    "    raw_flat[bbox_mask] = network_query_fn(pts)\n",
    "    raw = raw_flat.reshape(N_rays, N_samples, out_dim)\n",
    "    rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d)\n",
    "\n",
    "    ret = {'rgb_map': rgb_map, 'depth_map': depth_map, 'acc_map': acc_map}\n",
    "    if retraw:\n",
    "        ret['raw'] = raw\n",
    "    return ret\n",
    "\n",
    "\n",
    "def batchify_rays(rays_flat, chunk=1024 * 64):\n",
    "    \"\"\"Render rays in smaller minibatches to avoid OOM.\n",
    "    \"\"\"\n",
    "    all_ret = {}\n",
    "    for i in range(0, rays_flat.shape[0], chunk):\n",
    "        ret = render_rays(rays_flat[i:i + chunk])\n",
    "        for k in ret:\n",
    "            if k not in all_ret:\n",
    "                all_ret[k] = []\n",
    "            all_ret[k].append(ret[k])\n",
    "\n",
    "    all_ret = {k: torch.cat(all_ret[k], 0) for k in all_ret}\n",
    "    return all_ret\n",
    "\n",
    "\n",
    "def render(H, W, K, chunk=1024 * 64, rays=None, c2w=None, near=0., far=1., time_step=None):\n",
    "    if c2w is not None:\n",
    "        # special case to render full image\n",
    "        rays_o, rays_d = get_rays(H, W, K, c2w)\n",
    "    else:\n",
    "        # use provided ray batch\n",
    "        rays_o, rays_d = rays\n",
    "\n",
    "    sh = rays_d.shape  # [..., 3]\n",
    "\n",
    "    # Create ray batch\n",
    "    rays_o = torch.reshape(rays_o, [-1, 3]).float()\n",
    "    rays_d = torch.reshape(rays_d, [-1, 3]).float()\n",
    "\n",
    "    near, far = near * torch.ones_like(rays_d[..., :1]), far * torch.ones_like(rays_d[..., :1])\n",
    "    rays = torch.cat([rays_o, rays_d, near, far], -1)\n",
    "    time_step = time_step[:, None, None]  # [N_t, 1, 1]\n",
    "    N_t = time_step.shape[0]\n",
    "    N_r = rays.shape[0]\n",
    "    rays = torch.cat([rays[None].expand(N_t, -1, -1), time_step.expand(-1, N_r, -1)], -1)  # [N_t, n_rays, 7]\n",
    "    rays = rays.flatten(0, 1)  # [n_time_steps * n_rays, 7]\n",
    "\n",
    "    # Render and reshape\n",
    "    all_ret = batchify_rays(rays, chunk)\n",
    "    if N_t == 1:\n",
    "        for k in all_ret:\n",
    "            k_sh = list(sh[:-1]) + list(all_ret[k].shape[1:])\n",
    "            all_ret[k] = torch.reshape(all_ret[k], k_sh)\n",
    "\n",
    "    k_extract = ['rgb_map', 'depth_map', 'acc_map']\n",
    "    ret_list = [all_ret[k] for k in k_extract]\n",
    "    ret_dict = [{k: all_ret[k] for k in all_ret if k not in k_extract}, ]\n",
    "    return ret_list + ret_dict"
   ],
   "id": "20295871f3dcf3d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def render_path(render_poses, hwf, K, chunk = 512 * 64, gt_imgs=None, savedir=None, time_steps=None):\n",
    "    def merge_imgs(save_dir, framerate=30, prefix=''):\n",
    "        os.system(\n",
    "            'ffmpeg -hide_banner -loglevel error -y -i {0}/{1}%03d.png -vf palettegen {0}/palette.png'.format(save_dir,\n",
    "                                                                                                              prefix))\n",
    "        os.system(\n",
    "            'ffmpeg -hide_banner -loglevel error -y -framerate {0} -i {1}/{2}%03d.png -i {1}/palette.png -lavfi paletteuse {1}/_{2}.gif'.format(\n",
    "                framerate, save_dir, prefix))\n",
    "        os.system(\n",
    "            'ffmpeg -hide_banner -loglevel error -y -framerate {0} -i {1}/{2}%03d.png -i {1}/palette.png -lavfi paletteuse {1}/_{2}.mp4'.format(\n",
    "                framerate, save_dir, prefix))\n",
    "\n",
    "\n",
    "    H, W, focal = hwf\n",
    "    if time_steps is None:\n",
    "        time_steps = torch.ones(render_poses.shape[0], dtype=torch.float32)\n",
    "\n",
    "    rgbs = []\n",
    "    depths = []\n",
    "    psnrs = []\n",
    "    ssims = []\n",
    "    lpipss = []\n",
    "\n",
    "    lpips_net = lpips.LPIPS().cuda()\n",
    "\n",
    "    for i, c2w in enumerate(tqdm.tqdm(render_poses)):\n",
    "        rgb, depth, acc, _ = render(H, W, K, chunk=chunk, c2w=c2w[:3, :4], time_step=time_steps[i][None])\n",
    "        rgbs.append(rgb.cpu().numpy())\n",
    "        # normalize depth to [0,1]\n",
    "        depth = (depth - near) / (far - near)\n",
    "        depths.append(depth.cpu().numpy())\n",
    "\n",
    "        if gt_imgs is not None:\n",
    "            gt_img = torch.tensor(gt_imgs[i].squeeze(), dtype=torch.float32)  # [H, W, 3]\n",
    "            gt_img8 = to8b(gt_img.cpu().numpy())\n",
    "            gt_img = gt_img[90:960, 45:540]\n",
    "            rgb = rgb[90:960, 45:540]\n",
    "            lpips_value = lpips_net(rgb.permute(2, 0, 1), gt_img.permute(2, 0, 1), normalize=True).item()\n",
    "            p = -10. * np.log10(np.mean(np.square(rgb.detach().cpu().numpy() - gt_img.cpu().numpy())))\n",
    "            ssim_value = structural_similarity(gt_img.cpu().numpy(), rgb.cpu().numpy(), data_range=1.0, channel_axis=2)\n",
    "            lpipss.append(lpips_value)\n",
    "            psnrs.append(p)\n",
    "            ssims.append(ssim_value)\n",
    "            print(f'PSNR: {p:.4g}, SSIM: {ssim_value:.4g}, LPIPS: {lpips_value:.4g}')\n",
    "\n",
    "\n",
    "        if savedir is not None:\n",
    "            # save rgb and depth as a figure\n",
    "            rgb8 = to8b(rgbs[-1])\n",
    "            imageio.imsave(os.path.join(savedir, 'rgb_{:03d}.png'.format(i)), rgb8)\n",
    "            depth = depths[-1]\n",
    "            colored_depth_map = plt.cm.viridis(depth.squeeze())\n",
    "            imageio.imwrite(os.path.join(savedir, 'depth_{:03d}.png'.format(i)),\n",
    "                            (colored_depth_map * 255).astype(np.uint8))\n",
    "\n",
    "    if savedir is not None:\n",
    "        merge_imgs(savedir, prefix='rgb_')\n",
    "        merge_imgs(savedir, prefix='depth_')\n",
    "\n",
    "    rgbs = np.stack(rgbs, 0)\n",
    "    depths = np.stack(depths, 0)\n",
    "    if gt_imgs is not None:\n",
    "        avg_psnr = sum(psnrs) / len(psnrs)\n",
    "        avg_lpips = sum(lpipss) / len(lpipss)\n",
    "        avg_ssim = sum(ssims) / len(ssims)\n",
    "        print(\"Avg PSNR over Test set: \", avg_psnr)\n",
    "        print(\"Avg LPIPS over Test set: \", avg_lpips)\n",
    "        print(\"Avg SSIM over Test set: \", avg_ssim)\n",
    "        with open(os.path.join(savedir, \"test_psnrs_{:0.4f}_lpips_{:0.4f}_ssim_{:0.4f}.json\".format(avg_psnr, avg_lpips, avg_ssim)), 'w') as fp:\n",
    "            json.dump(psnrs, fp)\n",
    "\n",
    "    return rgbs, depths"
   ],
   "id": "b1d29fce989833a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in tqdm.trange(start, 10000 + 1):\n",
    "    # Sample random ray batch\n",
    "    batch_ray_idx = ray_idxs[i_batch:i_batch + N_rand]\n",
    "    batch_rays = rays[batch_ray_idx]  # [B, 2, 3]\n",
    "    batch_rays = torch.transpose(batch_rays, 0, 1)  # [2, B, 3]\n",
    "\n",
    "    i_batch += N_rand\n",
    "    # temporal bilinear sampling\n",
    "    time_idx = torch.randperm(T)[:N_time].float().to(device)  # [N_t]\n",
    "    time_idx += torch.randn(N_time) - 0.5  # -0.5 ~ 0.5\n",
    "    time_idx_floor = torch.floor(time_idx).long()\n",
    "    time_idx_ceil = torch.ceil(time_idx).long()\n",
    "    time_idx_floor = torch.clamp(time_idx_floor, 0, T - 1)\n",
    "    time_idx_ceil = torch.clamp(time_idx_ceil, 0, T - 1)\n",
    "    time_idx_residual = time_idx - time_idx_floor.float()\n",
    "    frames_floor = images_train[time_idx_floor]  # [N_t, VHW, 3]\n",
    "    frames_ceil = images_train[time_idx_ceil]  # [N_t, VHW, 3]\n",
    "    frames_interp = frames_floor * (1 - time_idx_residual).unsqueeze(-1) + \\\n",
    "                    frames_ceil * time_idx_residual.unsqueeze(-1)  # [N_t, VHW, 3]\n",
    "    time_step = time_idx / (T - 1) if T > 1 else torch.zeros_like(time_idx)\n",
    "    points = frames_interp[:, batch_ray_idx]  # [N_t, B, 3]\n",
    "    target_s = points.flatten(0, 1)  # [N_t*B, 3]\n",
    "\n",
    "    if i_batch >= rays.shape[0]:\n",
    "        print(\"Shuffle data after an epoch!\")\n",
    "        ray_idxs = torch.randperm(rays.shape[0])\n",
    "        i_batch = 0\n",
    "        resample_rays = True\n",
    "\n",
    "    #####  Core optimization loop  #####\n",
    "    rgb, depth, acc, extras = render(H, W, K, rays=batch_rays, time_step=time_step)\n",
    "\n",
    "    img_loss = img2mse(rgb, target_s)\n",
    "    loss = img_loss\n",
    "    psnr = mse2psnr(img_loss)\n",
    "    loss_meter.update(loss.item())\n",
    "    psnr_meter.update(psnr.item())\n",
    "\n",
    "    for param in grad_vars:  # slightly faster than optimizer.zero_grad()\n",
    "        param.grad = None\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    ###   update learning rate   ###\n",
    "    decay_rate = 0.1\n",
    "    decay_steps = lrate_decay\n",
    "    new_lrate = lrate * (decay_rate ** (global_step / decay_steps))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lrate\n",
    "    ################################\n",
    "\n",
    "    if i % 1000 == 0 and i > 0:\n",
    "        # Turn on testing mode\n",
    "        testsavedir = os.path.join(basedir, expname, 'spiral_{:06d}'.format(i))\n",
    "        os.makedirs(testsavedir, exist_ok=True)\n",
    "        with torch.no_grad():\n",
    "            render_path(render_poses, hwf, K, time_steps=render_timesteps, savedir=testsavedir)\n",
    "\n",
    "        testsavedir = os.path.join(basedir, expname, 'testset_{:06d}'.format(i))\n",
    "        os.makedirs(testsavedir, exist_ok=True)\n",
    "        with torch.no_grad():\n",
    "            test_view_pose = torch.tensor(poses_test[0])\n",
    "            N_timesteps = images_test.shape[0]\n",
    "            test_timesteps = torch.arange(N_timesteps) / (N_timesteps - 1)\n",
    "            test_view_poses = test_view_pose.unsqueeze(0).repeat(N_timesteps, 1, 1)\n",
    "            render_path(test_view_poses, hwf, K, time_steps=test_timesteps, gt_imgs=images_test,\n",
    "                        savedir=testsavedir)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        tqdm.tqdm.write(f\"[TRAIN] Iter: {i} Loss: {loss_meter.avg:.2g}  PSNR: {psnr_meter.avg:.4g}\")\n",
    "        loss_list.append(loss_meter.avg)\n",
    "        psnr_list.append(psnr_meter.avg)\n",
    "        loss_psnr = {\n",
    "            \"losses\": loss_list,\n",
    "            \"psnr\": psnr_list,\n",
    "        }\n",
    "        loss_meter.reset()\n",
    "        psnr_meter.reset()\n",
    "        with open(os.path.join(basedir, expname, \"loss_vs_time.json\"), \"w\") as fp:\n",
    "            json.dump(loss_psnr, fp)\n",
    "\n",
    "    if resample_rays:\n",
    "        print(\"Sampling new rays!\")\n",
    "        rays = []\n",
    "        ij = []\n",
    "        for p in poses_train[:, :3, :4]:\n",
    "            r_o, r_d, i_, j_ = get_rays_np_continuous(H, W, K, p)\n",
    "            rays.append([r_o, r_d])\n",
    "            ij.append([i_, j_])\n",
    "        rays = np.stack(rays, 0)  # [V, ro+rd=2, H, W, 3]\n",
    "        ij = np.stack(ij, 0)  # [V, 2, H, W]\n",
    "        images_train = sample_bilinear(images_train_, ij)  # [T, V, H, W, 3]\n",
    "        rays = np.transpose(rays, [0, 2, 3, 1, 4])  # [V, H, W, ro+rd=2, 3]\n",
    "        rays = np.reshape(rays, [-1, 2, 3])  # [VHW, ro+rd=2, 3]\n",
    "        rays = rays.astype(np.float32)\n",
    "\n",
    "        # Move training data to GPU\n",
    "        images_train = torch.Tensor(images_train).to(device).flatten(start_dim=1, end_dim=3)  # [T, VHW, 3]\n",
    "        T, S, _ = images_train.shape\n",
    "        rays = torch.Tensor(rays).to(device)\n",
    "\n",
    "        ray_idxs = torch.randperm(rays.shape[0])\n",
    "        i_batch = 0\n",
    "        resample_rays = False\n",
    "    global_step += 1\n"
   ],
   "id": "4badc8f97a987847",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
