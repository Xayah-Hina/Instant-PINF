{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-10T11:38:25.287085Z",
     "start_time": "2024-10-10T11:38:25.279445Z"
    }
   },
   "source": [
    "import argparse\n",
    "\n",
    "args = {'config': 'configs/scalar.txt',\n",
    "        'expname': 'scalar_test1',\n",
    "        'basedir': './log',\n",
    "        'datadir': './data/ScalarReal',\n",
    "        'net_model': 'siren',\n",
    "        'netdepth': 8,\n",
    "        'netwidth': 256,\n",
    "        'netdepth_fine': 8,\n",
    "        'netwidth_fine': 256,\n",
    "        'N_rand': 1024,\n",
    "        'lrate': 0.0005,\n",
    "        'lrate_decay': 500,\n",
    "        'chunk': 32768,\n",
    "        'netchunk': 65536,\n",
    "        'no_batching': True,\n",
    "        'no_reload': False,\n",
    "        'ft_path': None,\n",
    "        'fix_seed': 42,\n",
    "        'fading_layers': 50000,\n",
    "        'tempo_delay': 0,\n",
    "        'vel_delay': 10000,\n",
    "        'N_iter': 600000,\n",
    "        'train_warp': True,\n",
    "        'bbox_min': '0.05',\n",
    "        'bbox_max': '0.9',\n",
    "        'vgg_strides': 4,\n",
    "        'ghostW': 0.07,\n",
    "        'vggW': 0.01,\n",
    "        'overlayW': -0.0,\n",
    "        'd2vW': 2.0,\n",
    "        'nseW': 0.001,\n",
    "        'vol_output_only': False,\n",
    "        'vol_output_W': 128,\n",
    "        'render_only': False,\n",
    "        'render_test': False,\n",
    "        'N_samples': 64,\n",
    "        'N_importance': 64,\n",
    "        'perturb': 1.0,\n",
    "        'use_viewdirs': False,\n",
    "        'i_embed': -1,\n",
    "        'multires': 10,\n",
    "        'multires_views': 4,\n",
    "        'raw_noise_std': 0.0,\n",
    "        'render_factor': 0,\n",
    "        'precrop_iters': 1000,\n",
    "        'precrop_frac': 0.5,\n",
    "        'dataset_type': 'pinf_data',\n",
    "        'testskip': 20,\n",
    "        'shape': 'greek',\n",
    "        'white_bkgd': [1., 1., 1.],\n",
    "        'half_res': 'half',\n",
    "        'factor': 8,\n",
    "        'no_ndc': False,\n",
    "        'lindisp': False,\n",
    "        'spherify': False,\n",
    "        'llffhold': 8,\n",
    "        'i_print': 400,\n",
    "        'i_img': 2000,\n",
    "        'i_weights': 25000,\n",
    "        'i_testset': 50000,\n",
    "        'i_video': 50000}\n",
    "\n",
    "args = argparse.Namespace(**args)\n",
    "DEBUG = False"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T11:38:25.299508Z",
     "start_time": "2024-10-10T11:38:25.287085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import imageio.v2 as imageio\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Tuple, Union\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device('cuda')\n",
    "torch.set_default_dtype(torch.float32)"
   ],
   "id": "4e00c36b4f251b",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### generate pose spherical\n",
    "\n",
    "- pure torch functions\n",
    "- pure device functions"
   ],
   "id": "21d14598f506ecb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T11:38:25.328201Z",
     "start_time": "2024-10-10T11:38:25.319785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_translation_matrix(in_device: torch.device, in_t: float) -> torch.Tensor:\n",
    "    return torch.tensor([\n",
    "        [1., 0., 0., 0.],\n",
    "        [0., 1., 0., 0.],\n",
    "        [0., 0., 1., in_t],\n",
    "        [0., 0., 0., 1.]\n",
    "    ], dtype=torch.float32, device=in_device)\n",
    "\n",
    "\n",
    "def create_rotation_matrix_phi(in_device: torch.device, in_phi: float) -> torch.Tensor:\n",
    "    phi_tensor = torch.tensor(data=in_phi, dtype=torch.float32, device=in_device)\n",
    "    cos_phi = torch.cos(input=phi_tensor)\n",
    "    sin_phi = torch.sin(input=phi_tensor)\n",
    "    return torch.tensor(data=[\n",
    "        [1, 0, 0, 0],\n",
    "        [0, cos_phi, -sin_phi, 0],\n",
    "        [0, sin_phi, cos_phi, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ], dtype=torch.float32, device=in_device)\n",
    "\n",
    "\n",
    "def create_rotation_matrix_theta(in_device: torch.device, in_theta: float) -> torch.Tensor:\n",
    "    theta_tensor = torch.tensor(data=in_theta, dtype=torch.float32, device=in_device)\n",
    "    cos_theta = torch.cos(input=theta_tensor)\n",
    "    sin_theta = torch.sin(input=theta_tensor)\n",
    "    return torch.tensor(data=[\n",
    "        [cos_theta, 0, -sin_theta, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [sin_theta, 0, cos_theta, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ], dtype=torch.float32, device=in_device)\n",
    "\n",
    "\n",
    "def generate_pose_spherical(in_device: torch.device, in_theta: float, in_phi: float, in_radius: float,\n",
    "                            in_rotZ: bool, in_wx: float, in_wy: float, in_wz: float) -> torch.Tensor:\n",
    "    theta_rad = in_theta * torch.pi / 180.0\n",
    "    phi_rad = in_phi * torch.pi / 180.0\n",
    "\n",
    "    c2w = create_translation_matrix(in_device=in_device, in_t=in_radius)\n",
    "    c2w = create_rotation_matrix_phi(in_device=in_device, in_phi=phi_rad) @ c2w\n",
    "    c2w = create_rotation_matrix_theta(in_device=in_device, in_theta=theta_rad) @ c2w\n",
    "\n",
    "    if in_rotZ:\n",
    "        # Swap yz to keep right-hand coordinate system\n",
    "        swap_yz_matrix = torch.tensor(data=[\n",
    "            [-1, 0, 0, 0],\n",
    "            [0, 0, 1, 0],\n",
    "            [0, 1, 0, 0],\n",
    "            [0, 0, 0, 1]\n",
    "        ], dtype=torch.float32, device=in_device)\n",
    "        c2w = swap_yz_matrix @ c2w\n",
    "\n",
    "    translation = torch.tensor([\n",
    "        [1, 0, 0, in_wx],\n",
    "        [0, 1, 0, in_wy],\n",
    "        [0, 0, 1, in_wz],\n",
    "        [0, 0, 0, 1]\n",
    "    ], dtype=torch.float32, device=in_device)\n",
    "    c2w = translation @ c2w\n",
    "\n",
    "    return c2w"
   ],
   "id": "d8e234175c737d5d",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### load pinf frame data\n",
    "\n",
    "- pure numpy functions"
   ],
   "id": "4e291bb182d03311"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T11:39:03.735640Z",
     "start_time": "2024-10-10T11:38:25.348665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def resample_images(in_images: np.ndarray, in_factor: float) -> np.ndarray:\n",
    "    ret_images = np.zeros((in_images.shape[0], math.floor(in_images.shape[1] * in_factor),\n",
    "                           math.floor(in_images.shape[2] * in_factor), in_images.shape[3]))\n",
    "    for _ in range(in_images.shape[0]):\n",
    "        ret_images[_] = cv2.resize(in_images[_], (ret_images.shape[2], ret_images.shape[1]),\n",
    "                                   interpolation=cv2.INTER_AREA)\n",
    "    return ret_images\n",
    "\n",
    "\n",
    "def load_pinf_frame_data(in_type: str, in_skip: int, in_resample: float) -> Tuple[\n",
    "    np.ndarray, np.ndarray, np.ndarray, dict]:\n",
    "    all_images = []\n",
    "    all_poses = []\n",
    "    all_time_steps = []\n",
    "    ret_params = {}\n",
    "    with open(os.path.normpath(os.path.join(args.datadir, 'info.json')), 'r') as fp:\n",
    "        _meta = json.load(fp)\n",
    "        if (in_type + '_videos') not in _meta:\n",
    "            raise ValueError(f'No {in_type} videos found in the dataset')\n",
    "        for _video in _meta[in_type + '_videos']:\n",
    "            # load images, poses and time steps\n",
    "            _image_array = []\n",
    "            _pose_array = []\n",
    "            _time_step_array = []\n",
    "            _reader = imageio.get_reader(os.path.normpath(os.path.join(args.datadir, _video['file_name'])))\n",
    "            for _idx in range(0, _video['frame_num'], in_skip):\n",
    "                _reader.set_image_index(_idx)\n",
    "                _image_array.append(_reader.get_next_data())\n",
    "                _pose_array.append(_video['transform_matrix'])\n",
    "                _time_step_array.append(_idx * 1. / _video['frame_num'])\n",
    "            _reader.close()\n",
    "            all_images.append((np.array(_image_array) / 255.).astype(np.float32))\n",
    "            all_poses.append(np.array(_pose_array).astype(np.float32))\n",
    "            all_time_steps.append(np.array(_time_step_array).astype(np.float32))\n",
    "\n",
    "        ret_params['near'] = float(_meta['near']),\n",
    "        ret_params['far'] = float(_meta['far']),\n",
    "        ret_params['phi'] = float(_meta['phi']),\n",
    "        ret_params['rotZ'] = (_meta['rot'] == 'Z'),\n",
    "        ret_params['r_center'] = np.array(_meta['render_center']).astype(np.float32),\n",
    "        ret_params['bkg_color'] = np.array(_meta['frame_bkg_color']).astype(np.float32),\n",
    "\n",
    "    ret_images_np = np.concatenate(all_images, 0)\n",
    "    ret_poses_np = np.concatenate(all_poses, 0)\n",
    "    ret_time_steps_np = np.concatenate(all_time_steps, 0)\n",
    "\n",
    "    ret_images_resampled_np = resample_images(in_images=ret_images_np, in_factor=in_resample)\n",
    "    return ret_images_resampled_np, ret_poses_np, ret_time_steps_np, ret_params\n",
    "\n",
    "\n",
    "TRAIN_IMAGES, TRAIN_POSES, TRAIN_TIME_STEPS, TRAIN_PARAMS = load_pinf_frame_data(in_type='train', in_skip=1,\n",
    "                                                                                 in_resample=0.5 if args.half_res == 'half' else 1.0)\n",
    "VAR_IMAGES, VAR_POSES, VAR_TIME_STEPS, VAR_PARAMS = load_pinf_frame_data(in_type='train', in_skip=args.testskip,\n",
    "                                                                         in_resample=0.5 if args.half_res == 'half' else 1.0)\n",
    "TEST_IMAGES, TEST_POSES, TEST_TIME_STEPS, TEST_PARAMS = load_pinf_frame_data(in_type='train', in_skip=args.testskip,\n",
    "                                                                             in_resample=0.5 if args.half_res == 'half' else 1.0)"
   ],
   "id": "dec1410748c8238d",
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.95 GiB for an array with shape (600, 960, 540, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[80], line 52\u001B[0m\n\u001B[0;32m     48\u001B[0m     ret_images_resampled_np \u001B[38;5;241m=\u001B[39m resample_images(in_images\u001B[38;5;241m=\u001B[39mret_images_np, in_factor\u001B[38;5;241m=\u001B[39min_resample)\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret_images_resampled_np, ret_poses_np, ret_time_steps_np, ret_params\n\u001B[1;32m---> 52\u001B[0m TRAIN_IMAGES, TRAIN_POSES, TRAIN_TIME_STEPS, TRAIN_PARAMS \u001B[38;5;241m=\u001B[39m \u001B[43mload_pinf_frame_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_skip\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[43m                                                                                 \u001B[49m\u001B[43min_resample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhalf_res\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhalf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m VAR_IMAGES, VAR_POSES, VAR_TIME_STEPS, VAR_PARAMS \u001B[38;5;241m=\u001B[39m load_pinf_frame_data(in_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m, in_skip\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mtestskip,\n\u001B[0;32m     55\u001B[0m                                                                          in_resample\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mhalf_res \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhalf\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1.0\u001B[39m)\n\u001B[0;32m     56\u001B[0m TEST_IMAGES, TEST_POSES, TEST_TIME_STEPS, TEST_PARAMS \u001B[38;5;241m=\u001B[39m load_pinf_frame_data(in_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m, in_skip\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mtestskip,\n\u001B[0;32m     57\u001B[0m                                                                              in_resample\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mhalf_res \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhalf\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1.0\u001B[39m)\n",
      "Cell \u001B[1;32mIn[80], line 48\u001B[0m, in \u001B[0;36mload_pinf_frame_data\u001B[1;34m(in_type, in_skip, in_resample)\u001B[0m\n\u001B[0;32m     45\u001B[0m ret_poses_np \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(all_poses, \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     46\u001B[0m ret_time_steps_np \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(all_time_steps, \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 48\u001B[0m ret_images_resampled_np \u001B[38;5;241m=\u001B[39m \u001B[43mresample_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_images\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mret_images_np\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43min_resample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret_images_resampled_np, ret_poses_np, ret_time_steps_np, ret_params\n",
      "Cell \u001B[1;32mIn[80], line 2\u001B[0m, in \u001B[0;36mresample_images\u001B[1;34m(in_images, in_factor)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mresample_images\u001B[39m(in_images: np\u001B[38;5;241m.\u001B[39mndarray, in_factor: \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m----> 2\u001B[0m     ret_images \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_images\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloor\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_images\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43min_factor\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mmath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloor\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_images\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43min_factor\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_images\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(in_images\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[0;32m      5\u001B[0m         ret_images[_] \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(in_images[_], (ret_images\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m], ret_images\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]),\n\u001B[0;32m      6\u001B[0m                                    interpolation\u001B[38;5;241m=\u001B[39mcv2\u001B[38;5;241m.\u001B[39mINTER_AREA)\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 6.95 GiB for an array with shape (600, 960, 540, 3) and data type float64"
     ]
    }
   ],
   "execution_count": 80
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
