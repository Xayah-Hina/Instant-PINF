{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import taichi as ti\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import math\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import lpips\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "b7078d58b6c842e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args_npz = np.load(\"args.npz\", allow_pickle=True)\n",
    "args = SimpleNamespace(**{\n",
    "    key: value.item() if isinstance(value, np.ndarray) and value.size == 1 else\n",
    "    value.tolist() if isinstance(value, np.ndarray) else\n",
    "    value\n",
    "    for key, value in args_npz.items()\n",
    "})"
   ],
   "id": "2b37f6b833767b19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pinf_data = np.load(\"train_dataset.npz\")\n",
    "images_train_ = pinf_data['images_train']\n",
    "poses_train = pinf_data['poses_train']\n",
    "hwf = pinf_data['hwf']\n",
    "render_poses = pinf_data['render_poses']\n",
    "render_timesteps = pinf_data['render_timesteps']\n",
    "voxel_tran = pinf_data['voxel_tran']\n",
    "voxel_scale = pinf_data['voxel_scale']\n",
    "near = pinf_data['near'].item()\n",
    "far = pinf_data['far'].item()\n",
    "\n",
    "pinf_data_test = np.load(\"test_dataset.npz\")\n",
    "images_test = pinf_data_test['images_test']\n",
    "poses_test = pinf_data_test['poses_test']"
   ],
   "id": "7b7ceaa95904aaa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ti.init(arch=ti.cuda, device_memory_GB=8.0)\n",
    "from encoder import HashEncoderHyFluid\n",
    "\n",
    "max_res = np.array([args.finest_resolution, args.finest_resolution, args.finest_resolution, args.finest_resolution_t])\n",
    "min_res = np.array([args.base_resolution, args.base_resolution, args.base_resolution, args.base_resolution_t])\n",
    "embed_fn = HashEncoderHyFluid(min_res=min_res, max_res=max_res, num_scales=args.num_levels,\n",
    "                              max_params=2 ** args.log2_hashmap_size).to(device)\n",
    "input_ch = embed_fn.num_scales * 2  # default 2 params per scale\n",
    "embedding_params = list(embed_fn.parameters())"
   ],
   "id": "4f6ff72080e39126",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class NeRFSmall(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers=3,\n",
    "                 hidden_dim=64,\n",
    "                 geo_feat_dim=15,\n",
    "                 num_layers_color=2,\n",
    "                 hidden_dim_color=16,\n",
    "                 input_ch=3,\n",
    "                 ):\n",
    "        super(NeRFSmall, self).__init__()\n",
    "\n",
    "        self.input_ch = input_ch\n",
    "        self.rgb = torch.nn.Parameter(torch.tensor([0.0], device=device, dtype=torch.float32))\n",
    "\n",
    "        # sigma network\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.geo_feat_dim = geo_feat_dim\n",
    "\n",
    "        sigma_net = []\n",
    "        for l in range(num_layers):\n",
    "            if l == 0:\n",
    "                in_dim = self.input_ch\n",
    "            else:\n",
    "                in_dim = hidden_dim\n",
    "\n",
    "            if l == num_layers - 1:\n",
    "                out_dim = 1  # 1 sigma + 15 SH features for color\n",
    "            else:\n",
    "                out_dim = hidden_dim\n",
    "\n",
    "            sigma_net.append(torch.nn.Linear(in_dim, out_dim, bias=False, device=device, dtype=torch.float32))\n",
    "\n",
    "        self.sigma_net = torch.nn.ModuleList(sigma_net)\n",
    "\n",
    "        self.color_net = []\n",
    "        for l in range(num_layers_color):\n",
    "            if l == 0:\n",
    "                in_dim = 1\n",
    "            else:\n",
    "                in_dim = hidden_dim_color\n",
    "\n",
    "            if l == num_layers_color - 1:\n",
    "                out_dim = 1\n",
    "            else:\n",
    "                out_dim = hidden_dim_color\n",
    "\n",
    "            self.color_net.append(torch.nn.Linear(in_dim, out_dim, bias=True, device=device, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.sigma_net[l](h)\n",
    "            h = torch.nn.functional.relu(h, inplace=True)\n",
    "\n",
    "        sigma = h\n",
    "        return sigma\n",
    "\n",
    "\n",
    "model = NeRFSmall(num_layers=2,\n",
    "                  hidden_dim=64,\n",
    "                  geo_feat_dim=15,\n",
    "                  num_layers_color=2,\n",
    "                  hidden_dim_color=16,\n",
    "                  input_ch=input_ch).to(device)\n",
    "grad_vars = list(model.parameters())"
   ],
   "id": "99bbfe6ae7099dbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "\n",
    "        self.degenerated_to_sgd = degenerated_to_sgd\n",
    "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
    "            for param in params:\n",
    "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
    "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
    "                        buffer=[[None, None, None] for _ in range(10)])\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = group['buffer'][int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt(\n",
    "                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
    "                                    N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    elif self.degenerated_to_sgd:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = -1\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(p_data_fp32, alpha=-group['weight_decay'] * group['lr'])\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "                elif step_size > 0:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(p_data_fp32, alpha=-group['weight_decay'] * group['lr'])\n",
    "                    p_data_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "optimizer = RAdam([\n",
    "    {'params': grad_vars, 'weight_decay': 1e-6},\n",
    "    {'params': embedding_params, 'eps': 1e-15}\n",
    "], lr=args.lrate, betas=(0.9, 0.99))\n",
    "grad_vars += list(embedding_params)"
   ],
   "id": "1a980c9a5978fd58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "H, W, focal = hwf\n",
    "H, W = int(H), int(W)\n",
    "hwf = [H, W, focal]\n",
    "K = np.array([\n",
    "    [focal, 0, 0.5 * W],\n",
    "    [0, focal, 0.5 * H],\n",
    "    [0, 0, 1]\n",
    "])"
   ],
   "id": "64ee5db55bf65b3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def pos_world2smoke(Pworld, w2s, scale_vector):\n",
    "    pos_rot = torch.sum(Pworld[..., None, :] * (w2s[:3, :3]), -1)  # 4.world to 3.target\n",
    "    pos_off = (w2s[:3, -1]).expand(pos_rot.shape)  # 4.world to 3.target\n",
    "    new_pose = pos_rot + pos_off\n",
    "    pos_scale = new_pose / (scale_vector)  # 3.target to 2.simulation\n",
    "    return pos_scale\n",
    "\n",
    "\n",
    "class BBox_Tool(object):\n",
    "    def __init__(self, smoke_tran_inv, smoke_scale, in_min=[0.15, 0.0, 0.15], in_max=[0.85, 1., 0.85]):\n",
    "        self.s_w2s = torch.tensor(smoke_tran_inv, device=device, dtype=torch.float32).expand([4, 4])\n",
    "        self.s2w = torch.inverse(self.s_w2s)\n",
    "        self.s_scale = torch.tensor(smoke_scale.copy(), device=device, dtype=torch.float32).expand([3])\n",
    "        self.s_min = torch.tensor(in_min, device=device, dtype=torch.float32)\n",
    "        self.s_max = torch.tensor(in_max, device=device, dtype=torch.float32)\n",
    "\n",
    "    def world2sim(self, pts_world):\n",
    "        pts_world_homo = torch.cat([pts_world, torch.ones_like(pts_world[..., :1])], dim=-1)\n",
    "        pts_sim_ = torch.matmul(self.s_w2s, pts_world_homo[..., None]).squeeze(-1)[..., :3]\n",
    "        pts_sim = pts_sim_ / (self.s_scale)  # 3.target to 2.simulation\n",
    "        return pts_sim\n",
    "\n",
    "    def world2sim_rot(self, pts_world):\n",
    "        pts_sim_ = torch.matmul(self.s_w2s[:3, :3], pts_world[..., None]).squeeze(-1)\n",
    "        pts_sim = pts_sim_ / (self.s_scale)  # 3.target to 2.simulation\n",
    "        return pts_sim\n",
    "\n",
    "    def sim2world(self, pts_sim):\n",
    "        pts_sim_ = pts_sim * self.s_scale\n",
    "        pts_sim_homo = torch.cat([pts_sim_, torch.ones_like(pts_sim_[..., :1])], dim=-1)\n",
    "        pts_world = torch.matmul(self.s2w, pts_sim_homo[..., None]).squeeze(-1)[..., :3]\n",
    "        return pts_world\n",
    "\n",
    "    def sim2world_rot(self, pts_sim):\n",
    "        pts_sim_ = pts_sim * self.s_scale\n",
    "        pts_world = torch.matmul(self.s2w[:3, :3], pts_sim_[..., None]).squeeze(-1)\n",
    "        return pts_world\n",
    "\n",
    "    def isInside(self, inputs_pts):\n",
    "        target_pts = pos_world2smoke(inputs_pts, self.s_w2s, self.s_scale)\n",
    "        above = torch.logical_and(target_pts[..., 0] >= self.s_min[0], target_pts[..., 1] >= self.s_min[1])\n",
    "        above = torch.logical_and(above, target_pts[..., 2] >= self.s_min[2])\n",
    "        below = torch.logical_and(target_pts[..., 0] <= self.s_max[0], target_pts[..., 1] <= self.s_max[1])\n",
    "        below = torch.logical_and(below, target_pts[..., 2] <= self.s_max[2])\n",
    "        outputs = torch.logical_and(below, above)\n",
    "        return outputs\n",
    "\n",
    "    def insideMask(self, inputs_pts, to_float=True):\n",
    "        return self.isInside(inputs_pts).to(torch.float) if to_float else self.isInside(inputs_pts)\n",
    "\n",
    "\n",
    "voxel_tran_inv = np.linalg.inv(voxel_tran)\n",
    "bbox_model = BBox_Tool(voxel_tran_inv, voxel_scale)"
   ],
   "id": "10befeead07604b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_rays_np_continuous(H, W, K, c2w):\n",
    "    i, j = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy')\n",
    "    random_offset_i = np.random.uniform(0, 1, size=(H, W))\n",
    "    random_offset_j = np.random.uniform(0, 1, size=(H, W))\n",
    "    i = i + random_offset_i\n",
    "    j = j + random_offset_j\n",
    "    i = np.clip(i, 0, W - 1)\n",
    "    j = np.clip(j, 0, H - 1)\n",
    "\n",
    "    dirs = np.stack([(i - K[0][2]) / K[0][0], -(j - K[1][2]) / K[1][1], -np.ones_like(i)], -1)\n",
    "    # Rotate ray directions from camera frame to the world frame\n",
    "    rays_d = np.sum(dirs[..., np.newaxis, :] * c2w[:3, :3],\n",
    "                    -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n",
    "    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n",
    "    rays_o = np.broadcast_to(c2w[:3, -1], np.shape(rays_d))\n",
    "    return rays_o, rays_d, i, j\n",
    "\n",
    "\n",
    "def sample_bilinear(img, xy):\n",
    "    \"\"\"\n",
    "    Sample image with bilinear interpolation\n",
    "    :param img: (T, V, H, W, 3)\n",
    "    :param xy: (V, 2, H, W)\n",
    "    :return: img: (T, V, H, W, 3)\n",
    "    \"\"\"\n",
    "    T, V, H, W, _ = img.shape\n",
    "    u, v = xy[:, 0], xy[:, 1]\n",
    "\n",
    "    u = np.clip(u, 0, W - 1)\n",
    "    v = np.clip(v, 0, H - 1)\n",
    "\n",
    "    u_floor, v_floor = np.floor(u).astype(int), np.floor(v).astype(int)\n",
    "    u_ceil, v_ceil = np.ceil(u).astype(int), np.ceil(v).astype(int)\n",
    "\n",
    "    u_ratio, v_ratio = u - u_floor, v - v_floor\n",
    "    u_ratio, v_ratio = u_ratio[None, ..., None], v_ratio[None, ..., None]\n",
    "\n",
    "    bottom_left = img[:, np.arange(V)[:, None, None], v_floor, u_floor]\n",
    "    bottom_right = img[:, np.arange(V)[:, None, None], v_floor, u_ceil]\n",
    "    top_left = img[:, np.arange(V)[:, None, None], v_ceil, u_floor]\n",
    "    top_right = img[:, np.arange(V)[:, None, None], v_ceil, u_ceil]\n",
    "\n",
    "    bottom = (1 - u_ratio) * bottom_left + u_ratio * bottom_right\n",
    "    top = (1 - u_ratio) * top_left + u_ratio * top_right\n",
    "\n",
    "    interpolated = (1 - v_ratio) * bottom + v_ratio * top\n",
    "\n",
    "    return interpolated\n",
    "\n",
    "\n",
    "rays_list = []\n",
    "ij = []\n",
    "for p in poses_train[:, :3, :4]:\n",
    "    r_o, r_d, i_, j_ = get_rays_np_continuous(H, W, K, p)\n",
    "    rays_list.append([r_o, r_d])\n",
    "    ij.append([i_, j_])\n",
    "rays_np = np.stack(rays_list, 0)  # [V, ro+rd=2, H, W, 3]\n",
    "ij = np.stack(ij, 0)  # [V, 2, H, W]\n",
    "images_train_sample = sample_bilinear(images_train_, ij)  # [T, V, H, W, 3]\n",
    "\n",
    "rays_np = np.transpose(rays_np, [0, 2, 3, 1, 4])  # [V, H, W, ro+rd=2, 3]\n",
    "rays_np = np.reshape(rays_np, [-1, 2, 3])  # [VHW, ro+rd=2, 3]\n",
    "rays_np = rays_np.astype(np.float32)\n",
    "\n",
    "images_train_gpu = torch.tensor(images_train_sample, device=device, dtype=torch.float32).flatten(start_dim=1, end_dim=3)\n",
    "T, S, _ = images_train_gpu.shape\n",
    "rays_gpu = torch.tensor(rays_np, device=device, dtype=torch.float32)\n",
    "ray_idxs_gpu = torch.randperm(rays_gpu.shape[0], device=device, dtype=torch.int32)\n",
    "print(f'images_train: {images_train_gpu.shape}, rays: {rays_gpu.shape}, T: {T}, S: {S}')"
   ],
   "id": "f1434810da5f3636",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "img2mse = lambda x, y: torch.mean((x - y) ** 2)\n",
    "mse2psnr = lambda x: -10. * torch.log(x) / torch.log(torch.tensor([10.], device=device))\n",
    "to8b = lambda x: (255 * np.clip(x, 0, 1)).astype(np.uint8)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    val = 0\n",
    "    avg = 0\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    tot_count = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.tot_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = float(val)\n",
    "        self.sum += float(val) * n\n",
    "        self.count += n\n",
    "        self.tot_count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "loss_list = []\n",
    "psnr_list = []\n",
    "loss_meter, psnr_meter = AverageMeter(), AverageMeter()\n",
    "resample_rays = False"
   ],
   "id": "b1c709f843eeda47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start = 0\n",
    "i_batch = 0\n",
    "# for i in trange(start + 1, args.N_iters + 1):\n",
    "for i in trange(start + 1, start + 2):\n",
    "    batch_ray_idx = ray_idxs_gpu[i_batch:i_batch + args.N_rand]\n",
    "    batch_rays = torch.transpose(rays_gpu[batch_ray_idx], 0, 1)\n",
    "\n",
    "    time_idx = torch.randperm(T, device=device, dtype=torch.float32)[:args.N_time]\n",
    "    time_idx += torch.randn(args.N_time, device=device, dtype=torch.float32) - 0.5\n",
    "    time_idx_floor = torch.floor(time_idx).long()\n",
    "    time_idx_ceil = torch.ceil(time_idx).long()\n",
    "    time_idx_floor = torch.clamp(time_idx_floor, 0, T - 1)\n",
    "    time_idx_ceil = torch.clamp(time_idx_ceil, 0, T - 1)\n",
    "    time_idx_residual = time_idx - time_idx_floor.float()\n",
    "    frames_floor = images_train_gpu[time_idx_floor]\n",
    "    frames_ceil = images_train_gpu[time_idx_ceil]\n",
    "    frames_interp = frames_floor * (1 - time_idx_residual).unsqueeze(-1) + frames_ceil * time_idx_residual.unsqueeze(-1)\n",
    "    time_step = time_idx / (T - 1) if T > 1 else torch.zeros_like(time_idx)\n",
    "    points = frames_interp[:, batch_ray_idx]\n",
    "    target_s = points.flatten(0, 1)\n",
    "\n",
    "    i_batch += args.N_rand\n",
    "    if i_batch >= rays_gpu.shape[0]:\n",
    "        print(\"Shuffle data after an epoch!\")\n",
    "        ray_idxs_gpu = torch.randperm(rays_gpu.shape[0], device=device)\n",
    "        i_batch = 0\n",
    "        resample_rays = True\n",
    "\n",
    "    #####################################################################################################################################\n",
    "    _rays_o, _rays_d = batch_rays\n",
    "    _near_tensor, _far_tensor = near * torch.ones_like(_rays_d[..., :1]), far * torch.ones_like(_rays_d[..., :1])\n",
    "    _rays = torch.cat([_rays_o, _rays_d, _near_tensor, _far_tensor], -1)\n",
    "\n",
    "    _t_vals = torch.linspace(0., 1., steps=args.N_samples, device=device, dtype=torch.float32)\n",
    "    _z_vals = _near_tensor * (1. - _t_vals) + _far_tensor * (_t_vals)\n",
    "\n",
    "    _mids = .5 * (_z_vals[..., 1:] + _z_vals[..., :-1])\n",
    "    _upper = torch.cat([_mids, _z_vals[..., -1:]], -1)\n",
    "    _lower = torch.cat([_z_vals[..., :1], _mids], -1)\n",
    "    _t_rand = torch.rand(_z_vals.shape, device=device, dtype=torch.float32)\n",
    "    _z_vals = _lower + (_upper - _lower) * _t_rand\n",
    "\n",
    "    pts = _rays_o[..., None, :] + _rays_d[..., None, :] * _z_vals[..., :, None]\n",
    "    time_step_expanded = time_step.expand(pts.shape[0], pts.shape[1], 1)\n",
    "    pts_with_time = torch.cat([pts, time_step_expanded], dim=-1)\n",
    "    pts_with_time_flat = torch.reshape(pts_with_time, [-1, pts_with_time.shape[-1]])\n",
    "\n",
    "    out_dim = 1\n",
    "    raw_flat = torch.zeros([pts_with_time_flat.shape[0], out_dim], device=device, dtype=torch.float32)\n",
    "\n",
    "    bbox_mask = bbox_model.insideMask(pts_with_time_flat[..., :3], to_float=False)\n",
    "    if bbox_mask.sum() == 0:\n",
    "        bbox_mask[0] = True\n",
    "    pts_final = pts_with_time_flat[bbox_mask]\n",
    "    raw_flat[bbox_mask] = model(embed_fn(pts_final))\n",
    "    raw = raw_flat.reshape(pts_with_time.shape[0], pts_with_time.shape[1], out_dim)\n",
    "\n",
    "    raw2alpha = lambda raw, dists, act_fn=torch.nn.functional.relu: 1. - torch.exp(-act_fn(raw) * dists)\n",
    "    dists = _z_vals[..., 1:] - _z_vals[..., :-1]\n",
    "    dists = torch.cat([dists, torch.tensor([1e10], device=device).expand(dists[..., :1].shape)], -1)\n",
    "    dists = dists * torch.norm(_rays_d[..., None, :], dim=-1)\n",
    "    rgb = torch.ones(3, device=device) * (0.6 + torch.tanh(model.rgb) * 0.4)\n",
    "    noise = 0.\n",
    "    alpha = raw2alpha(raw[..., -1] + noise, dists)\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1), device=device), 1. - alpha + 1e-10], -1),\n",
    "                                    -1)[:, :-1]\n",
    "    rgb_map = torch.sum(weights[..., None] * rgb, -2)\n",
    "    depth_map = torch.sum(weights * _z_vals, -1) / (torch.sum(weights, -1) + 1e-10)\n",
    "    disp_map = 1. / torch.max(1e-10 * torch.ones_like(depth_map), depth_map)\n",
    "    acc_map = torch.sum(weights, -1)\n",
    "    depth_map[acc_map < 1e-1] = 0.\n",
    "\n",
    "    ret = {\n",
    "        'rgb_map': rgb_map,\n",
    "        'disp_map': disp_map,\n",
    "        'acc_map': acc_map,\n",
    "        'weights': weights,\n",
    "        'depth_map': depth_map,\n",
    "    }\n",
    "\n",
    "    #####################################################################################################################################\n",
    "    img_loss = img2mse(rgb_map, target_s)\n",
    "    loss = img_loss\n",
    "    psnr = mse2psnr(img_loss)\n",
    "    loss_meter.update(loss.item())\n",
    "    psnr_meter.update(psnr.item())\n",
    "\n",
    "    for param in grad_vars:  # slightly faster than optimizer.zero_grad()\n",
    "        param.grad = None\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    decay_rate = 0.1\n",
    "    decay_steps = args.lrate_decay\n",
    "    new_lrate = args.lrate * (decay_rate ** (i / decay_steps))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lrate\n",
    "\n",
    "    os.makedirs(os.path.join(args.basedir, args.expname), exist_ok=True)\n",
    "\n",
    "    if i % args.i_print == 0:\n",
    "        tqdm.write(f\"[TRAIN] Iter: {i} Loss: {loss_meter.avg:.2g}  PSNR: {psnr_meter.avg:.4g}\")\n",
    "        loss_list.append(loss_meter.avg)\n",
    "        psnr_list.append(psnr_meter.avg)\n",
    "        loss_psnr = {\n",
    "            \"losses\": loss_list,\n",
    "            \"psnr\": psnr_list,\n",
    "        }\n",
    "        loss_meter.reset()\n",
    "        psnr_meter.reset()\n",
    "\n",
    "    if resample_rays:\n",
    "        print(\"Sampling new rays!\")\n",
    "        rays_list = []\n",
    "        ij = []\n",
    "        for p in poses_train[:, :3, :4]:\n",
    "            r_o, r_d, i_, j_ = get_rays_np_continuous(H, W, K, p)\n",
    "            rays_list.append([r_o, r_d])\n",
    "            ij.append([i_, j_])\n",
    "        rays_np = np.stack(rays_list, 0)  # [V, ro+rd=2, H, W, 3]\n",
    "        ij = np.stack(ij, 0)  # [V, 2, H, W]\n",
    "        images_train_sample = sample_bilinear(images_train_, ij)  # [T, V, H, W, 3]\n",
    "        rays_np = np.transpose(rays_np, [0, 2, 3, 1, 4])  # [V, H, W, ro+rd=2, 3]\n",
    "        rays_np = np.reshape(rays_np, [-1, 2, 3])  # [VHW, ro+rd=2, 3]\n",
    "        rays_np = rays_np.astype(np.float32)\n",
    "\n",
    "        # Move training data to GPU\n",
    "        images_train_gpu = torch.tensor(images_train_sample, device=device, dtype=torch.float32).flatten(start_dim=1,\n",
    "                                                                                                         end_dim=3)\n",
    "        T, S, _ = images_train_gpu.shape\n",
    "        rays_gpu = torch.tensor(rays_np, device=device, dtype=torch.float32)\n",
    "        ray_idxs_gpu = torch.randperm(rays_gpu.shape[0], device=device, dtype=torch.int32)\n",
    "\n",
    "        i_batch = 0\n",
    "        resample_rays = False"
   ],
   "id": "b1bd561802c9f3e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_rays(H, W, K, c2w):\n",
    "    i, j = torch.meshgrid(torch.linspace(0, W - 1, W), torch.linspace(0, H - 1, H),\n",
    "                          indexing='ij')  # pytorch's meshgrid has indexing='ij'\n",
    "    i = i.t()\n",
    "    j = j.t()\n",
    "    dirs = torch.stack([(i - K[0][2]) / K[0][0], -(j - K[1][2]) / K[1][1], -torch.ones_like(i)], -1)\n",
    "    # Rotate ray directions from camera frame to the world frame\n",
    "    rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3, :3],\n",
    "                       -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n",
    "    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n",
    "    rays_o = c2w[:3, -1].expand(rays_d.shape)\n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n",
    "def merge_imgs(save_dir, framerate=30, prefix=''):\n",
    "    os.system(\n",
    "        'ffmpeg -hide_banner -loglevel error -y -i {0}/{1}%03d.png -vf palettegen {0}/palette.png'.format(save_dir,\n",
    "                                                                                                          prefix))\n",
    "    os.system(\n",
    "        'ffmpeg -hide_banner -loglevel error -y -framerate {0} -i {1}/{2}%03d.png -i {1}/palette.png -lavfi paletteuse {1}/_{2}.gif'.format(\n",
    "            framerate, save_dir, prefix))\n",
    "    os.system(\n",
    "        'ffmpeg -hide_banner -loglevel error -y -framerate {0} -i {1}/{2}%03d.png -i {1}/palette.png -lavfi paletteuse {1}/_{2}.mp4'.format(\n",
    "            framerate, save_dir, prefix))\n",
    "\n",
    "\n",
    "os.makedirs(os.path.join(\"output\"), exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    test_view_pose = torch.tensor(poses_test[0], device=device, dtype=torch.float32)\n",
    "    N_timesteps = images_test.shape[0]\n",
    "    test_timesteps = torch.arange(N_timesteps) / (N_timesteps - 1)\n",
    "    test_view_poses = test_view_pose.unsqueeze(0).repeat(N_timesteps, 1, 1)\n",
    "\n",
    "    rgbs = []\n",
    "    depths = []\n",
    "    psnrs = []\n",
    "    ssims = []\n",
    "    lpipss = []\n",
    "    lpips_net = lpips.LPIPS().cuda()\n",
    "\n",
    "    for i, c2w in enumerate(tqdm(test_view_pose)):\n",
    "        _rays_o, _rays_d = get_rays(H, W, K, c2w)\n",
    "        _rays_o = torch.reshape(_rays_o, [-1, 3]).float()\n",
    "        _rays_d = torch.reshape(_rays_d, [-1, 3]).float()\n",
    "\n",
    "        _near_tensor, _far_tensor = near * torch.ones_like(_rays_d[..., :1]), far * torch.ones_like(_rays_d[..., :1])\n",
    "        _rays = torch.cat([_rays_o, _rays_d, _near_tensor, _far_tensor], -1)\n",
    "\n",
    "        _t_vals = torch.linspace(0., 1., steps=args.N_samples, device=device, dtype=torch.float32)\n",
    "        _z_vals = _near_tensor * (1. - _t_vals) + _far_tensor * (_t_vals)\n",
    "\n",
    "        _mids = .5 * (_z_vals[..., 1:] + _z_vals[..., :-1])\n",
    "        _upper = torch.cat([_mids, _z_vals[..., -1:]], -1)\n",
    "        _lower = torch.cat([_z_vals[..., :1], _mids], -1)\n",
    "        _t_rand = torch.rand(_z_vals.shape, device=device, dtype=torch.float32)\n",
    "        _z_vals = _lower + (_upper - _lower) * _t_rand\n",
    "\n",
    "        pts = _rays_o[..., None, :] + _rays_d[..., None, :] * _z_vals[..., :, None]\n",
    "        time_step_expanded = time_step.expand(pts.shape[0], pts.shape[1], 1)\n",
    "        pts_with_time = torch.cat([pts, time_step_expanded], dim=-1)\n",
    "        pts_with_time_flat = torch.reshape(pts_with_time, [-1, pts_with_time.shape[-1]])\n",
    "\n",
    "        out_dim = 1\n",
    "        raw_flat = torch.zeros([pts_with_time_flat.shape[0], out_dim], device=device, dtype=torch.float32)\n",
    "\n",
    "        bbox_mask = bbox_model.insideMask(pts_with_time_flat[..., :3], to_float=False)\n",
    "        if bbox_mask.sum() == 0:\n",
    "            bbox_mask[0] = True\n",
    "        pts_final = pts_with_time_flat[bbox_mask]\n",
    "        raw_flat[bbox_mask] = model(embed_fn(pts_final))\n",
    "        raw = raw_flat.reshape(pts_with_time.shape[0], pts_with_time.shape[1], out_dim)\n",
    "\n",
    "        raw2alpha = lambda raw, dists, act_fn=torch.nn.functional.relu: 1. - torch.exp(-act_fn(raw) * dists)\n",
    "        dists = _z_vals[..., 1:] - _z_vals[..., :-1]\n",
    "        dists = torch.cat([dists, torch.tensor([1e10], device=device).expand(dists[..., :1].shape)], -1)\n",
    "        dists = dists * torch.norm(_rays_d[..., None, :], dim=-1)\n",
    "        rgb = torch.ones(3, device=device) * (0.6 + torch.tanh(model.rgb) * 0.4)\n",
    "        noise = 0.\n",
    "        alpha = raw2alpha(raw[..., -1] + noise, dists)\n",
    "        weights = alpha * torch.cumprod(\n",
    "            torch.cat([torch.ones((alpha.shape[0], 1), device=device), 1. - alpha + 1e-10], -1),\n",
    "            -1)[:, :-1]\n",
    "        rgb_map = torch.sum(weights[..., None] * rgb, -2)\n",
    "        depth_map = torch.sum(weights * _z_vals, -1) / (torch.sum(weights, -1) + 1e-10)\n",
    "        disp_map = 1. / torch.max(1e-10 * torch.ones_like(depth_map), depth_map)\n",
    "        acc_map = torch.sum(weights, -1)\n",
    "        depth_map[acc_map < 1e-1] = 0.\n",
    "\n",
    "        ret = {\n",
    "            'rgb_map': rgb_map,\n",
    "            'disp_map': disp_map,\n",
    "            'acc_map': acc_map,\n",
    "            'weights': weights,\n",
    "            'depth_map': depth_map,\n",
    "        }\n",
    "\n",
    "        rgbs.append(rgb_map.cpu().numpy())\n",
    "        depth_map = (depth_map - near) / (far - near)\n",
    "        depths.append(depth_map.cpu().numpy())\n",
    "\n",
    "        rgb8 = to8b(rgbs[-1])\n",
    "        imageio.imsave(os.path.join(\"output\", 'rgb_{:03d}.png'.format(i)), rgb8)\n",
    "        depth = depths[-1]\n",
    "        colored_depth_map = plt.cm.viridis(depth.squeeze())\n",
    "        imageio.imwrite(os.path.join(\"output\", 'depth_{:03d}.png'.format(i)),\n",
    "                        (colored_depth_map * 255).astype(np.uint8))\n",
    "    merge_imgs(\"output\", prefix='rgb_')\n",
    "    merge_imgs(\"output\", prefix='depth_')"
   ],
   "id": "a37e5b1efb0ee18d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
